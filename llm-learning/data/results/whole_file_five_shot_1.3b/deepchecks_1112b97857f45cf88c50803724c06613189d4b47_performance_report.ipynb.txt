```python
    # Fix string labels in performance report / simple model comparison
    # The code above is a simple example of how to fix string labels in a performance report.
    # In this example, we're using the Deepchecks library to compare two models.
    # The performance report is created using the `run` method of the `PerformanceReport` class.
    # The `run` method takes three arguments: the training dataset, the test dataset, and the model.
    # The `run` method returns a dictionary with the performance metrics of the model.
    # The performance metrics are then used to create a performance report.
    # The performance report is a visualization of the performance of the model.
    # The performance report shows the accuracy, precision, recall, and F1-score of the model.
    # The performance report also shows the confusion matrix of the model.
    # The performance report is a good way to compare the performance of two or more models.

    # To fix string labels in the performance report, you can use the `set_labels` method of the `PerformanceReport` class.
    # The `set_labels` method takes a dictionary as an argument, where the keys are the labels of the features, and the values are the new labels.
    # For example, if you have a feature named 'target' and you want to change it to 'label', you can do:

    check.set_labels({'target': 'label'})

    # After setting the labels, you can run the performance report again:

    check.run(train_ds, test_ds, clf)

    # This will create a performance report with the labels changed to 'label'.
    # The performance report will now show the labels as 'label' instead of 'target'.

    # Note: The `set_labels` method only changes the labels in the performance report.
    # It does not change the labels in the original data.
    # If you want to change the labels in the original data, you can use the `set_features` method of the `Dataset` class.
    # The `set_features` method takes a dictionary as an argument, where the keys are the old labels, and the values are the new labels.
    # For example, if you have a feature named
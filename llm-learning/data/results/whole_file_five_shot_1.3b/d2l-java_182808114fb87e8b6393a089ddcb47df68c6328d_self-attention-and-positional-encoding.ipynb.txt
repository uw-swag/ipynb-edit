```python
    class SelfAttention(nn.Module):
        def __init__(self, n_heads, d_model):
            super(SelfAttention, self).__init__()
            self.n_heads = n_heads
            self.d_model = d_model
            self.depth = d_model // n_heads
            self.wq = nn.Linear(d_model, d_model)
            self.wk = nn.Linear(d_model, d_model)
            self.wv = nn.Linear(d_model, d_model)
            self.dense = nn.Linear(d_model, d_model)

        def split_heads(self, x, batch_size):
            x = tf.reshape(x, (batch_size, -1, self.n_heads, self.depth))
            return tf.transpose(x, perm=[0, 2, 1, 3])

        def forward(self, v, k, q, mask=None):
            batch_size = q.shape[0]

            q = self.wq(q)
            k = self.wk(k)
            v = self.wv(v)

            q = self.split_heads(q, batch_size)
            k = self.split_heads(k, batch_size)
            v = self.split_heads(v, batch_size)

            scores = tf.matmul(q, k.transpose_checkpoint(True, False, True, False))

            if mask is not None:
                scores = scores + mask

            attention_weights = tf.nn.softmax(scores, axis=-1)
            output = tf.matmul(attention_weights, v)
            output = tf.transpose(output, perm=[0, 2, 1, 3])
            output = output.reshape(batch_size, -1, self.d_model)

            output = self.dense(output)
            return output, attention_weights
    ```

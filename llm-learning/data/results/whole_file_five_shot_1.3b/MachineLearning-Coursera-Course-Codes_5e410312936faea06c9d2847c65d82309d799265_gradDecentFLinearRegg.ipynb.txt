```python
    import numpy as np

    class LinearRegression:
        def __init__(self, learning_rate=0.01, iterations=1000):
            self.learning_rate = learning_rate
            self.iterations = iterations
            self.weights = None
            self.bias = None

        def fit(self, X, y):
            num_samples, num_features = X.shape

            # Initialize weights and bias
            self.weights = np.zeros(num_features)
            self.bias = 0

            # Gradient Descent
            for _ in range(self.iterations):
                y_predicted = np.dot(X, self.weights) + self.bias

                # Compute gradients
                dw = (1 / num_samples) * np.dot(X.T, (y_predicted - y))
                db = (1 / num_samples) * np.sum(y_predicted - y)

                # Update weights and bias
                self.weights -= self.learning_rate * dw
                self.bias -= self.learning_rate * db

        def predict(self, X):
            return np.dot(X, self.weights) + self.bias
    ```

    This is a simple implementation of gradient descent for linear regression. The `fit` method implements the gradient descent algorithm, and the `predict` method uses the trained weights to make predictions.

    The learning rate and the number of iterations are hyperparameters that can be tuned to optimize the model's performance.

    The gradient descent algorithm works by iteratively updating the weights and bias in the direction that reduces the error of the model.

    The error of a model is calculated as the average squared difference between the actual and predicted values. The gradient of the error with respect to the weights and bias is computed, and then used to update the weights and bias.

    The learning rate determines the step size at each iteration, and the number of iterations determines the number of times the algorithm is run.

    The `fit` method returns the trained model, which can be used to make predictions on new data.

    The `predict` method
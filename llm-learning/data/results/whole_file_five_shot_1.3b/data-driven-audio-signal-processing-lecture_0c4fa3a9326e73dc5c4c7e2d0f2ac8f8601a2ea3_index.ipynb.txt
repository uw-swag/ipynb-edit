```python
    import numpy as np

    def compute_gradient(f, w):
        return np.array([(f(w + h) - f(w)) / h for h in np.linspace(-1, 1, 3)])

    def gradient_descent(f, initial_w, learning_rate, num_iterations):
        w = initial_w
        for _ in range(num_iterations):
            grad = compute_gradient(f, w)
            w -= learning_rate * grad
        return w

    def f(w):
        return w**2

    initial_w = 0.5
    learning_rate = 0.01
    num_iterations = 1000

    w = gradient_descent(f, initial_w, learning_rate, num_iterations)
    print(f"The minimum occurs at w = {w}")
    ```
    This code demonstrates the univariate version of gradient descent. It computes the gradient of a function at a given point, and then uses this gradient to update the point. The process is repeated for a specified number of iterations. The final value of the point is printed out.

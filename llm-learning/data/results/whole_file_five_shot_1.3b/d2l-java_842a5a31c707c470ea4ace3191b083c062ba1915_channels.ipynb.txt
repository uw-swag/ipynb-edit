```python
    <Cell_0>
    # set the environment path to find Recommenders
    import sys
    sys.path.append("../../")
    import pyspark
    from pyspark.ml.recommendation import ALS
    import pyspark.sql.functions as F
    from pyspark.sql import SparkSession
    from pyspark.sql.types import StructType, StructField
    from pyspark.sql.types import StringType, FloatType, IntegerType, LongType

    from reco_utils.common.timer import Timer
    from reco_utils.dataset import movielens
    from reco_utils.common.notebook_utils import is_jupyter
    from reco_utils.dataset.spark_splitters import spark_random_split
    from reco_utils.evaluation.spark_evaluation import SparkRatingEvaluation, SparkRankingEvaluation
    from reco_utils.common.spark_utils import start_or_get_spark

    from reco_utils.evaluation.diversity_evaluator import DiversityEvaluator
    from pyspark.sql.window import Window

    import numpy as np
    import pandas as pd

    print("System version: {}".format(sys.version))
    print("Spark version: {}".format(pyspark.__version__))

    <\Cell_0>
    <Cell_1>
    # top k items to recommend
    TOP_K = 10

    # Select MovieLens data size: 100k, 1m, 10m, or 20m
    MOVIELENS_DATA_SIZE = '100k'

    # user, item column names
    COL_USER="UserId"
    COL_ITEM="MovieId"
    COL_RATING="Rating"
    <\Cell_1>
    <Cell_2>
    # the following settings work well for debugging locally on VM - change when running on a cluster
    # set up a giant single executor with many threads and specify memory cap

    spark = start_or_get

    '''
    ## 14.6. Subword Embedding (#122)

    Subword Embedding is a technique used in NLP to represent words as a combination of smaller subwords. This is done by splitting the words into subwords and then using these subwords to represent the words.

    The subword embedding is a way to represent words in a way that is more meaningful and less redundant than the word embedding. This is done by using a subword tokenizer, which splits the words into subwords. The subword embedding is then used to represent the words.

    The subword embedding is a crucial part of the BERT model, which is used for many NLP tasks.

    ## 14.6.1. Subword Tokenization

    Subword tokenization is the process of splitting a word into subwords. The subwords are usually of a length between 2 and 30 characters.

    ## 14.6.2. Subword Embedding

    Subword embedding is the process of representing a subword as a vector. The vector is a dense vector that represents the subword.

    ## 14.6.3. Subword Indexing

    Subword indexing is the process of converting a subword to its corresponding index in the subword embedding.

    ## 14.6.4. Subword Embedding in BERT

    In BERT, the subword embedding is used to represent the subwords in the input. The subword embeddings are then used to represent the words in the input.

    '''

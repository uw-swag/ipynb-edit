```python
    # Section 8.5: Implementation of Recurrent Neural Networks from Scratch (#88)
    # ----------------------------------------

    # 8.5.1: Forward Propagation
    def forward_propagation(self, X):
        self.layers[0].A = X
        for layer_num in range(1, len(self.layers)):
            self.layers[layer_num].A = self.sigmoid(self.layers[layer_num].W.dot(self.layers[layer_num - 1].A) + self.layers[layer_num].b)
        return self.layers[-1].A

    # 8.5.2: Backward Propagation
    def backward_propagation(self, y, overestimation):
        self.layers[-1].dA = (self.layers[-1].A - y) * overestimation
        for layer_num in range(len(self.layers) - 2, -1, -1):
            self.layers[layer_num].dA = self.layers[layer_num + 1].dA.dot(self.layers[layer_num + 1].W.T) * self.sigmoid_derivative(self.layers[layer_num].A)
        self.layers[0].dB = self.layers[0].dA.dot(self.layers[1].W.T)

    # 8.5.3: Update Weights
    def update_weights(self, learning_rate):
        for layer_num in range(len(self.layers) - 1, -1, -1):
            self.layers[layer_num].W += self.layers[layer_num].dA.dot(self.layers[layer_num].A.T) * learning_rate
            self.layers[layer_num].b += self.layers[layer_num].dB * learning_rate

    # 8.5.4: Train
    def train(self, X, y, learning_rate, epochs):
        for _ in range(epochs):
            self.forward_propagation(X)
            self.
```python
    import numpy as np

    class SoftmaxRegression:
        def __init__(self, num_features, num_classes, learning_rate=0.01, epochs=1000):
            self.num_features = num_features
            self.num_classes = num_classes
            self.learning_rate = learning_rate
            self.epochs = epochs
            self.weights = np.zeros(shape=(num_features, num_classes))
            self.bias = np.zeros(shape=(num_classes,))

        def fit(self, X, y):
            for _ in range(self.epochs):
                linear_output = np.dot(X, self.weights) + self.bias
                probabilities = self._softmax(linear_output)
                gradient = np.dot(X.T, (probabilities - y)) / y.shape[0]
                self.weights -= self.learning_rate * gradient.T
                self.bias -= self.learning_rate * np.sum(gradient, axis=0, keepdims=True)

        def predict(self, X):
            linear_output = np.dot(X, self.weights) + self.bias
            probabilities = self._softmax(linear_output)
            predicted_class = np.argmax(probabilities, axis=1)
            return predicted_class

        def _softmax(self, x):
            e_x = np.exp(x - np.max(x))
            return e_x / e_x.sum(axis=0, keepdims=True)

        def _one_hot_encode(self, y):
            y_one_hot = np.zeros(shape=(y.shape[0], self.num_classes))
            y_one_hot[np.arange(y.shape[0]), y] = 1
            return y_one_hot

    # Example usage:
    X = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])
    y
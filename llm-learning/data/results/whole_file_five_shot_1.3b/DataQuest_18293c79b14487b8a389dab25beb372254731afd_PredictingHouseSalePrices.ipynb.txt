
    ### Instruction:
    [
    Commit Message: "Implemented feature engineering for better model performance"
    Original Code Cells:
    '''
    <Cell_0>
%matplotlib inline
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error
<\Cell_0>
<Cell_1>
data = pd.read_csv('AmesHousing.txt', delimiter='\t')
<\Cell_1>
<Cell_2>
# Implement feature engineering
df = transform_features(data, missing_values_percent_threshold=0.05)
features, target = select_features(df, min_correlation=0.25, min_variance=0.0015, max_unique_vals=10)
df = df[features + [target]]
<\Cell_2>
<Cell_3>
# Split the data into training and testing sets
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(df.drop('SalePrice', axis=1), df['SalePrice'], test_size=0.25, random_state=1)
<\Cell_3>
<Cell_4>
# Train a linear regression model
lr = LinearRegression()
lr.fit(X_train, y_train)
predictions = lr.predict(X_test)
mse = mean_squared_error(y_test, predictions)
rmse = np.sqrt(mse)
rmse
<\Cell_4>
<Cell_5>
# Verify the model performance
lr.score(X_test, y_test)
<\Cell_5>
<Cell_6>
# Check the importance of the features
feature_importances = pd.DataFrame({'feature': X_train.columns, 'importance': lr.coef_})
feature_importances.
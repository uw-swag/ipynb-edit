```
    import pandas as pd
    import sklearn
    from sklearn.preprocessing import LabelEncoder
    from sklearn.model_selection import train_test_split
    from sklearn.ensemble import RandomForestClassifier
    from sklearn.metrics import log_loss
    from sklearn.calibration import CalibratedClassifierCV
    import numpy as np
    import xgboost as xgb

    #Read data. Input the path to the files instead of "../input".
    data = pd.read_csv('../input/train.csv')
    test = pd.read_csv('../input/test.csv')

    #Define data for modelling.
    X_train = data.drop('id', axis=1)
    X_train = X_train.drop('target', axis=1)
    Y_train = LabelEncoder().fit_transform(data.target.values)
    X_test = test.drop('id', axis=1)

    #For crossvalidation
    Xtrain, Xtest, ytrain, ytest = train_test_split(X_train, Y_train, test_size=0.20, random_state=36)

    clf = RandomForestClassifier(n_estimators=300, n_jobs=-1, criterion = 'gini')
    #CalibratedClassifierCV - probability calibration with cross-validation.
    calibrated_clf = CalibratedClassifierCV(clf, method='isotonic', cv=5)
    calibrated_clf.fit(Xtrain, ytrain)
    y_val = calibrated_clf.predict_proba(Xtest)
    y_submit = calibrated_clf.predict_proba(X_test)
    print("Loss on validation set: ", log_loss(ytest, y_val, eps=1e-15, normalize=True))

    #I decided to add XGBoost to improve the model and it helped.
    params = {"objective": "multi:softprob", "num_class": 9}
    gbm = xgb.train
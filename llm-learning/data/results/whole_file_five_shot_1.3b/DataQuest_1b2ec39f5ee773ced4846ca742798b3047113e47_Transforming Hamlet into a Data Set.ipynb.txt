```python
    # Find path to PySpark
    import findspark
    findspark.init()

    # Import PySpark & initalize SparkContext object
    import pyspark
    sc = pyspark.SparkContext()
    raw_hamlet=sc.textFile('hamlet.txt')
    raw_hamlet.take(10)

    # Split RDD
    split_hamlet = raw_hamlet.map(lambda line: line.split('\t'))
    split_hamlet.take(10)

    # Remove hamlet@ from the row id's
    def format_id(line):
        id = line[0].split('@')[1]
        result = []
        result.append(id)
        if len(line) > 1:
            for y in line[1:]:
                result.append(y)
        return result

    hamlet_with_ids = split_hamlet.map(lambda line: format_id(line))

    hamlet_with_ids.take(10)
    ```

```python
    # Section 2.5: Automatic Differentiation (#80)
    # This section covers the use of automatic differentiation in Python.
    # We will use the `jax` library, which is a library for automatic differentiation in Python.
    # It is compatible with the `numpy` library and can be used to compute gradients.

    # First, we need to install the `jax` library. You can do this by running the following command in your notebook:
    # !pip install --upgrade jax

    # Then, we can use the `jax.grad` function to compute the gradient of a function.
    # Here is an example:

    import jax.numpy as jnp

    def f(x):
        return jnp.sum(x**2)

    x = jnp.ones((3,))
    print(jax.grad(f)(x))
    ```
    # Note: The `jax` library is not available in Colab. You may need to run the above code in your local environment.
    ```

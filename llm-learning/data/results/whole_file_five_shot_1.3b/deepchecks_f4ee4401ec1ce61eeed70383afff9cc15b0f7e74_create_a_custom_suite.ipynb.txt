```
    # importing all existing checks for demonstration simplicity
    from deepchecks.checks import *
    from deepchecks import Suite
    from sklearn.metrics import precision_score, recall_score, make_scorer

    # The Suite's first argument is its name, and then all of the check objects.
    # Some checks can receive arguments when initialized (all check arguments have default values)
    # Each check can have an optional condition(/s)
    # Multiple conditions can be applied subsequentially
    new_custom_suite = Suite('Simple Suite For Model Performance',
        ModelInfo(),
        # use custom scorers for performance report:
        PerformanceReport().add_condition_train_test_relative_degradation_not_greater_than(threshold=0.15\
                     ).add_condition_test_performance_not_less_than(0.8),
        ConfusionMatrixReport(),
        SimpleModelComparison(simple_model_type='constant', \
                              alternative_scorers={'Recall (Multiclass)': make_scorer(recall_score, average=None), \
                                                   'Precision (Multiclass)': make_scorer(precision_score, average=None)} \
                             ).add_condition_gain_not_less_than(0.3)
    )
    # Let's see the suite:
    new_custom_suite

    # General imports
    import pandas as pd
    import numpy as np
    np.random.seed(22)

    from deepchecks.datasets.classification import iris
    from sklearn.ensemble import RandomForestClassifier
    from sklearn.model_selection import train_test_split

    # Load pre-split Datasets
    train_dataset, test_dataset = iris.load_data(as_train_test=True)
    label_col = 'target'

    # Train Model
    rf_clf = RandomForestClassifier()
    rf_clf.fit(train_dataset.features_columns,train_dataset.label_col);

    new_custom_suite.run(model=rf_
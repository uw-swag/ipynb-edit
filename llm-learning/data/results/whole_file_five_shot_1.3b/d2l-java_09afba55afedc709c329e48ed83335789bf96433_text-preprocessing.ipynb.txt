
    This commit introduces the first set of tutorials on using the Deep Java Library (DJL) for natural language processing tasks. The tutorial covers the basics of using the DJL API, including reading from a time machine dataset, tokenizing text, and creating a vocabulary.

    The tutorial also covers the process of loading a time machine dataset, which is a dataset of text lines from a file, tokenizing each line, and creating a vocabulary from the tokens. The vocabulary is a mapping from tokens to indices, and the indices are used to represent the tokens in the corpus.

    The tutorial also covers the process of getting the indices of a list of tokens, and the process of getting the size of the corpus and the size of the vocabulary.

    The tutorial also includes a function to load a time machine dataset, which is a dataset of text lines from a file, tokenizes each line, and creates a vocabulary from the tokens. The vocabulary is a mapping from tokens to indices, and the indices are used to represent the tokens in the corpus.

    The tutorial also includes a function to load a time machine dataset, which is a dataset of text lines from a file, tokenizes each line, and creates a vocabulary from the tokens. The vocabulary is a mapping from tokens to indices, and the indices are used to represent the tokens in the corpus.

    The tutorial also includes a function to load a time machine dataset, which is a dataset of text lines from a file, tokenizes each line, and creates a vocabulary from the tokens. The vocabulary is a mapping from tokens to indices, and the indices are used to represent the tokens in the corpus.

    The tutorial also includes a function to load a time machine dataset, which is a dataset of text lines from a file, tokenizes each line, and creates a vocabulary from the tokens. The vocabulary is a mapping from tokens to indices, and the indices are used to represent the tokens in the corpus.

    The tutorial also includes a function to load a time machine dataset, which is a dataset of text lines from a file, tokenizes each line, and creates a vocabulary from the tokens. The vocabulary is a mapping from tokens to indices, and the indices are used to represent the tokens in the corpus.

    The tutorial also includes a function to load a time machine dataset, which is a dataset
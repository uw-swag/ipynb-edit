```python
    '''
    import random
    from keras.layers import Dense, Dropout
    from keras.models import Sequential
    from keras.optimizers import SGD
    import numpy as np
    import pickle
    import json
    import nltk
    from nltk.stem import WordNetLemmatizer
    import pandas as pd

    lemmatizer = WordNetLemmatizer()
    nltk.download('omw-1.4')
    nltk.download("punkt")
    nltk.download("wordnet")

    words = []
    classes = []
    documents = []
    ignore_words = ["?", "!", ","]

    data_file = open("../intents.json").read()
    intents = json.loads(data_file)

    for intent in intents["intent"]:
        for pattern in intent["patterns"]:
            w = nltk.word_tokenize(pattern)
            words.extend(w)

            documents.append((w, intent["tag"]))
            if intent["tag"] not in classes:
                classes.append(intent["tag"])

    words = [lemmatizer.lemmatize(w.lower()) for w in words if w not in ignore_words]
    words = sorted(list(set(words)))

    classes = sorted(list(set(classes)))

    training = []

    output_empty = [0] * len(classes)

    for doc in documents:
        bag = []
        pattern_words = doc[0]
        pattern_words = [lemmatizer.lemmatize(word.lower()) for word in pattern_words]
        bag.extend(pattern_words)

        output_row = list(output_empty)
        output_row[classes.index(doc[1])] = 1
        training.append([bag, output_row])

    random.shuffle(training)

    train_x = list(training[:, 0])
    train_y = list(training[:, 1])

    model = Sequential()
   
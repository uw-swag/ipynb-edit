
    This commit includes a bump of the D2L Java book version to 0.8.0. The bump is related to the version of the ai.djl library, which is used in the code.

    The code also includes a new method `initAdadeltaStates`, which initializes the states for the AdaDelta optimization algorithm.

    The `adadelta` method is a new method in the `Optimization` class, which implements the AdaDelta optimization algorithm.

    The `trainAdadelta` method is a new method in the `TrainingChapter11` class, which trains the AdaDelta optimizer on the given data.

    The `Optimizer adadelta = Optimizer.adadelta().optRho(0.9f).build();` line creates an instance of the AdaDelta optimizer with a learning rate of 0.9.

    The `TrainingChapter11.trainConciseCh11(adadelta, airfoil, 2);` line trains the AdaDelta optimizer on the given data for 2 epochs.

    The code also includes a new method `Optimizer.adadelta().optRho(0.9f).build();`, which creates an instance of the AdaDelta optimizer with a learning rate of 0.9.

    The `TrainingChapter11.trainConciseCh11(adadelta, airfoil, 2);` line trains the AdaDelta optimizer on the given data for 2 epochs.

    The code also includes a new method `Optimizer.adadelta().optRho(0.9f).build();`, which creates an instance of the AdaDelta optimizer with a learning rate of 0.9.

    The `TrainingChapter11.trainConciseCh11(adadelta, airfoil, 2);` line trains the AdaDelta optimizer on the given data for 2 epochs.

    The code also includes a new method `Optimizer.adadelta().optRho(0.9f).build();`, which creates an instance of the AdaDelta optimizer with a learning rate of 0.9.


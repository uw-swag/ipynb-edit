```python
    from sklearn.metrics import mean_average_precision_score
    from sklearn.metrics import precision_recall_curve
    from sklearn.metrics import average_precision_score
    from sklearn.metrics import precision_recall_fscore_support

    def MeanAveragePrecisionReport(y_true, y_pred):
        # Compute precision@k
        precision = dict()
        for k in range(1, 6):
            precision[k] = precision_recall_fscore_support(y_true, y_pred, average='binary', verbose=0,
                                                           pos_label=1, sample_weight=None, bins=10)
            if k == 1:
                print("1-class precision: {:.2f}".format(precision[k][0]))
            else:
                print("{}-class precision: {:.2f}".format(k, precision[k][0]))

        # Compute average precision
        average_precision = average_precision_score(y_true, y_pred, average='binary')
        print('Average precision-recall score: {0:0.2f}'.format(average_precision))

        # Compute mean average precision
        mean_average_precision = mean_average_precision_score(y_true, y_pred, average='binary')
        print('Mean average precision: {0:0.2f}'.format(mean_average_precision))

        # Compute precision-recall curve
        precision["micro"], recall["micro"], _ = precision_recall_curve(y_true, y_pred, average='binary')
        plt.step(recall["micro"], precision["micro"], color='b', alpha=0.2, where='post')
        plt.fill_between(recall["micro"], precision["micro"], step='post', alpha=0.2, color='b')
        plt.xlabel('Recall')
        plt.ylabel('Precision')
        plt.title('2-class Precision-Recall curve: AP
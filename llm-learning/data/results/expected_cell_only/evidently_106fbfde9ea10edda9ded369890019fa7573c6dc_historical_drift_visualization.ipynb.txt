content = requests.get("https://archive.ics.uci.edu/ml/machine-learning-databases/00275/Bike-Sharing-Dataset.zip").content
with zipfile.ZipFile(io.BytesIO(content)) as arc:
    raw_data = pd.read_csv(arc.open("day.csv"), header=0, sep=',', parse_dates=['dteday'])
#set column mapping for Evidently Profile
data_columns = ColumnMapping()
data_columns.datetime = 'dteday'
data_columns.numerical_features = ['weathersit', 'temp', 'atemp', 'hum', 'windspeed']
features_historical_drift = []

for date in experiment_batches:
    drifts = detect_features_drift(raw_data.loc[raw_data.dteday.between(reference_dates[0],reference_dates[1])], 
                           raw_data.loc[raw_data.dteday.between(date[0], date[1])], 
                           column_mapping=data_columns)
    
    features_historical_drift.append([x[1] for x in drifts])
    
features_historical_drift_frame = pd.DataFrame(features_historical_drift, 
                                               columns = data_columns.numerical_features)
features_historical_drift_pvalues = []

for date in experiment_batches:
    drifts = detect_features_drift(raw_data.loc[raw_data.dteday.between(reference_dates[0], reference_dates[1])], 
                           raw_data.loc[raw_data.dteday.between(date[0], date[1])],
                           column_mapping=data_columns,
                           get_scores=True)
    
    features_historical_drift_pvalues.append([x[1] for x in drifts])
    
features_historical_drift_pvalues_frame = pd.DataFrame(features_historical_drift_pvalues, 
                                                       columns = data_columns.numerical_features)

fig = go.Figure(data=go.Heatmap(
                   z = features_historical_drift_pvalues_frame.transpose(),
                   x = [x[1] for x in experiment_batches],
                   y = features_historical_drift_pvalues_frame.columns,
                   hoverongaps = False,
                   xgap = 1,
                   ygap = 1,
                   zmin = 0,
                   zmax = 1,
                   colorscale = 'reds_r'
                   )
               )

fig.update_xaxes(side="top")

fig.update_layout(
    xaxis_title = "Timestamp",
    yaxis_title = "p-value"
)

fig.show("notebook")
dataset_historical_drift = []

for date in experiment_batches:
    dataset_historical_drift.append(detect_dataset_drift(raw_data.loc[raw_data.dteday.between(reference_dates[0], reference_dates[1])], 
                           raw_data.loc[raw_data.dteday.between(date[0], date[1])], 
                           column_mapping=data_columns))
dataset_historical_drift_ratio = []

for date in experiment_batches:
    dataset_historical_drift_ratio.append(detect_dataset_drift(raw_data.loc[raw_data.dteday.between(reference_dates[0], reference_dates[1])], 
                           raw_data.loc[raw_data.dteday.between(date[0],date[1])],
                           column_mapping=data_columns,
                           get_ratio=True))
#log into MLflow
client = MlflowClient()

#set experiment
mlflow.set_experiment('Dataset Drift Analysis with Evidently')

#start new run
for date in experiment_batches:
    with mlflow.start_run() as run: 
        
        # Log parameters
        mlflow.log_param("begin", date[0])
        mlflow.log_param("end", date[1])

        # Log metrics
        metric = detect_dataset_drift(raw_data.loc[raw_data.dteday.between(reference_dates[0], reference_dates[1])], 
                           raw_data.loc[raw_data.dteday.between(date[0], date[1])],
                           column_mapping=data_columns,
                           get_ratio=True)
        
        mlflow.log_metric('dataset drift', metric)

        print(run.info)

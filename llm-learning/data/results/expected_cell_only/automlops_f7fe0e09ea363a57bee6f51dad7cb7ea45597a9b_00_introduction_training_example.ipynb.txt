# Copyright 2024 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
PROJECT_ID = '[your-project-id]'
TRAINING_DATASET = f'{PROJECT_ID}.test_dataset.dry-beans'
TARGET_COLUMN = 'Class'
@AutoMLOps.component(
    packages_to_install=[
        'google-cloud-aiplatform'
    ]
)
def deploy_model(
    model_directory: str,
    project_id: str,
    region: str
):
    """Custom component that uploads a saved model from GCS to Vertex Model Registry
       and deploys the model to an endpoint for online prediction.

    Args:
        model_directory: GS location of saved model.
        project_id: Project_id.
        region: Region.
    """
    import pprint as pp
    import random

    from google.cloud import aiplatform

    aiplatform.init(project=project_id, location=region)
    # Check if model exists
    models = aiplatform.Model.list()
    model_name = 'beans-model'
    if 'beans-model' in (m.name for m in models):
        parent_model = model_name
        model_id = None
        is_default_version=False
        version_aliases=['experimental', 'challenger', 'custom-training', 'decision-tree']
        version_description='challenger version'
    else:
        parent_model = None
        model_id = model_name
        is_default_version=True
        version_aliases=['champion', 'custom-training', 'decision-tree']
        version_description='first version'

    serving_container = 'us-docker.pkg.dev/vertex-ai/prediction/sklearn-cpu.1-2:latest'
    uploaded_model = aiplatform.Model.upload(
        artifact_uri=model_directory,
        model_id=model_id,
        display_name=model_name,
        parent_model=parent_model,
        is_default_version=is_default_version,
        version_aliases=version_aliases,
        version_description=version_description,
        serving_container_image_uri=serving_container,
        serving_container_ports=[8080],
        labels={'created_by': 'automlops-team'},
    )

    endpoint = uploaded_model.deploy(
        machine_type='n1-standard-4',
        deployed_model_display_name='deployed-beans-model')

    sample_input = [[random.uniform(0, 300) for x in range(16)]]

    # Test endpoint predictions
    print('running prediction test...')
    try:
        resp = endpoint.predict(instances=sample_input)
        pp.pprint(resp)
    except Exception as ex:
        print('prediction request failed', ex)
import datetime
pipeline_params = {
    'bq_table': TRAINING_DATASET,
    'model_directory': f'gs://{PROJECT_ID}-{MODEL_ID}-bucket/trained_models/{datetime.datetime.now()}',
    'data_path': f'gs://{PROJECT_ID}-{MODEL_ID}-bucket/data.csv',
    'project_id': PROJECT_ID,
    'region': 'us-central1'
}
AutoMLOps.generate(project_id=PROJECT_ID,
                   pipeline_params=pipeline_params,
                   use_ci=True,
                   naming_prefix=MODEL_ID,
                   schedule_pattern='59 11 * * 0', # retrain every Sunday at Midnight
                   setup_model_monitoring=True     # use this if you would like to use Vertex Model Monitoring
)
AutoMLOps.provision(hide_warnings=False)           # hide_warnings is optional, defaults to True
from google.cloud import aiplatform

aiplatform.init(project=PROJECT_ID)
beans_endpoints = aiplatform.Endpoint.list(filter=f'display_name="beans-model_endpoint"')

# Grab the most recent beans-model deployment
endpoint_name = beans_endpoints[0].resource_name
endpoint_name
AutoMLOps.monitor(
    alert_emails=[], # update if you would like to receive email alerts
    target_field=TARGET_COLUMN,
    model_endpoint=endpoint_name,
    monitoring_interval=1,
    auto_retraining_params=pipeline_params,
    drift_thresholds={'Area': 0.0001, 'Perimeter': 0.0001},
    skew_thresholds={'Area': 0.0001, 'Perimeter': 0.0001},
    training_dataset=f'bq://{TRAINING_DATASET}'
)
from google.cloud import bigquery
import pandas as pd

def get_query(bq_input_table: str) -> str:
    """Generates BQ Query to read data.

    Args:
        bq_input_table: The full name of the bq input table to be read into
        the dataframe (e.g. <project>.<dataset>.<table>)

    Returns: A BQ query string.
    """
    return f'''SELECT * FROM `{bq_input_table}`'''

def load_bq_data(query: str, client: bigquery.Client) -> pd.DataFrame:
    """Loads data from bq into a Pandas Dataframe for EDA.

    Args:
        query: BQ Query to generate data.
        client: BQ Client used to execute query.

    Returns:
        pd.DataFrame: A dataframe with the requested data.
    """
    df = client.query(query).to_dataframe()
    return df

bq_client = bigquery.Client(project=PROJECT_ID)    

# Get samples
df = load_bq_data(get_query(TRAINING_DATASET), bq_client)
X_sample = df.iloc[:,:-1][:2000].values.tolist()

endpoint = aiplatform.Endpoint(endpoint_name)
response = endpoint.predict(instances=X_sample)
prediction = response[0]
# print the first prediction
print(prediction[0])
AutoMLOps.generate(project_id=PROJECT_ID, # required
                   pipeline_params=pipeline_params, # required
                   artifact_repo_location='us-central1', # default
                   artifact_repo_name=None, # default
                   artifact_repo_type='artifact-registry', # default
                   base_image='python:3.9-slim', # default
                   build_trigger_location='us-central1', # default
                   build_trigger_name=None, # default
                   custom_training_job_specs=None, # default
                   deployment_framework='cloud-build', # default
                   naming_prefix='automlops-default-prefix', # default
                   orchestration_framework='kfp', # default
                   pipeline_job_runner_service_account=None, # default
                   pipeline_job_submission_service_location='us-central1', # default
                   pipeline_job_submission_service_name=None, # default
                   pipeline_job_submission_service_type='cloud-functions', # default
                   project_number=None, # default
                   provision_credentials_key=None, # default
                   provisioning_framework='gcloud', # default
                   pubsub_topic_name=None, # default
                   schedule_location='us-central1', # default
                   schedule_name=None, # default
                   schedule_pattern='No Schedule Specified', # default
                   setup_model_monitoring=False, # default
                   source_repo_branch='automlops', # default
                   source_repo_name=None, # default
                   source_repo_type='cloud-source-repositories', # default
                   storage_bucket_location='us-central1', # default
                   storage_bucket_name=None, # default
                   use_ci=False, # default
                   vpc_connector='No VPC Specified', # default
                   workload_identity_pool=None, # default
                   workload_identity_provider=None, # default
                   workload_identity_service_account=None, # default
)
from google.cloud import aiplatform

aiplatform.init(project=PROJECT_ID)
beans_endpoints = aiplatform.Endpoint.list(filter=f'display_name="beans-model_endpoint"')

# Grab the most recent beans-model deployment
endpoint_name = beans_endpoints[0].resource_name
endpoint_name
AutoMLOps.monitor(
    alert_emails=[], # update if you would like to receive email alerts
    target_field=TARGET_COLUMN,
    model_endpoint=endpoint_name,
    monitoring_interval=1,
    auto_retraining_params=pipeline_params,
    drift_thresholds={'Area': 0.0001, 'Perimeter': 0.0001},
    skew_thresholds={'Area': 0.0001, 'Perimeter': 0.0001},
    training_dataset=f'bq://{TRAINING_DATASET}'
)

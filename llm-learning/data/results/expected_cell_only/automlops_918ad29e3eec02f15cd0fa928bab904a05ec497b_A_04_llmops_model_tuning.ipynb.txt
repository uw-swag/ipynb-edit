# Copyright 2023 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
!pip install google-cloud-aiplatform google-cloud-bigquery --upgrade --user
# Automatically restart kernel after installs so that your environment can access the new packages
# import IPython

# app = IPython.Application.instance()
# app.kernel.do_shutdown(True)
# from google.colab import auth
# auth.authenticate_user()
PROJECT_ID = "<your-project-id>"  # @param {type:"string"}

# Set the project id
! gcloud config set project {PROJECT_ID}
import random
import string

# Generate a uuid of a specifed length(default=8)
def generate_uuid(length: int = 8) -> str:
    return "".join(random.choices(string.ascii_lowercase + string.digits, k=length))

UUID = generate_uuid()
BUCKET_NAME = "<your_bucket_name>"  # @param {type:"string"}
BUCKET_URI = f"gs://{BUCKET_NAME}"
REGION = "us-central1"  # @param {type: "string"}
if BUCKET_NAME == "" or BUCKET_NAME is None or BUCKET_NAME == "[your-bucket-name]":
    BUCKET_NAME = "vertex-" + UUID
    BUCKET_URI = f"gs://{BUCKET_NAME}"
! gsutil mb -l $REGION -p $PROJECT_ID $BUCKET_URI
! gsutil ls -al $BUCKET_URI
# import vertexai

# PROJECT_ID = "[your-project-id]"  # @param {type:"string"}
# vertexai.init(project=PROJECT_ID, location="us-central1")
from typing import Union

import pandas as pd
from sklearn.model_selection import train_test_split
import numpy as np

from vertexai.preview.language_models import TextGenerationModel
from google.cloud import aiplatform
from google.cloud import bigquery
def run_bq_query(sql: str) -> Union[str, pd.DataFrame]:
    """
    Run a BigQuery query and return the job ID or result as a DataFrame
    Args:
        sql: SQL query, as a string, to execute in BigQuery
    Returns:
        df: DataFrame of results from query,  or error, if any
    """

    bq_client = bigquery.Client()

    # Try dry run before executing query to catch any errors
    job_config = bigquery.QueryJobConfig(dry_run=True, use_query_cache=False)
    bq_client.query(sql, job_config=job_config)

    # If dry run succeeds without errors, proceed to run query
    job_config = bigquery.QueryJobConfig()
    client_result = bq_client.query(sql, job_config=job_config)

    job_id = client_result.job_id

    # Wait for query/job to finish running. then get & return data frame
    df = client_result.result().to_arrow().to_pandas()
    print(f"Finished job_id: {job_id}")
    return df
df = run_bq_query("""SELECT
    CONCAT(q.title, q.body) as input_text,
    a.body AS output_text
FROM
    `bigquery-public-data.stackoverflow.posts_questions` q
JOIN
    `bigquery-public-data.stackoverflow.posts_answers` a
ON
    q.accepted_answer_id = a.id
WHERE
    q.accepted_answer_id IS NOT NULL AND
    REGEXP_CONTAINS(q.tags, "python") AND
    a.creation_date >= "2020-01-01"
LIMIT
    10000
""")

df.head()
df['input_text'][10]
df['output_text'][10]
print(len(df))
train, evaluation = train_test_split(df, test_size=0.2)
print(len(train))
tune_jsonl = train.to_json(orient='records', lines=True)

print(f"Length: {len(tune_jsonl)}")

print(tune_jsonl[0:100])
training_data_filename = "tune_data_stack_overflow_python_qa.jsonl"

with open(training_data_filename, "w") as f:
    f.write(df_jsonl)
! gsutil cp $training_data_filename $BUCKET_URI
! gsutil ls -al $BUCKET_URI
TRAINING_DATA_URI = f"{BUCKET_URI}/{training_data_filename}"
MODEL_NAME= f"genai-workshop-tuned-model-{UUID}"
def tuned_model(
    project_id: str,
    location: str,
    training_data: str,
    model_display_name: str,
    train_steps = 100,
):

    """Prompt-tune a new model, based on a prompt-response data.

    "training_data" can be either the GCS URI of a file formatted in JSONL format
    (for example: training_data=f'gs://{bucket}/{filename}.jsonl'), or a pandas
    DataFrame. Each training example should be JSONL record with two keys, for
    example:
      {
        "input_text": <input prompt>,
        "output_text": <associated output>
      },
    or the pandas DataFame should contain two columns:
      ['input_text', 'output_text']
    with rows for each training example.

    Args:
      project_id: GCP Project ID, used to initialize aiplatform
      location: GCP Region, used to initialize aiplatform
      training_data: GCS URI of training file or pandas dataframe of training data
      train_steps: Number of training steps to use when tuning the model.
    """

    aiplatform.init(project=project_id, location=location)
    model = TextGenerationModel.from_pretrained("text-bison@001")

    model.tune_model(
        training_data=training_data,
        model_display_name=model_display_name,
        train_steps=train_steps,
        # Tuning can only happen in the "europe-west4" location
        tuning_job_location="europe-west4",
        # Model can only be deployed in the "us-central1" location
        tuned_model_location="us-central1",
    )

    # Test the tuned model:
    print(model.predict("Can you provide me with a Python implementation of BERT with Tensorflow? Example: "))

    return model
# This will start the tuning job and output a URL where you can monitor the pipeline execution.
model = tuned_model(PROJECT_ID, REGION, TRAINING_DATA_URI, MODEL_NAME)
def list_tuned_models(project_id, location):

    aiplatform.init(project=project_id, location=location)
    model = TextGenerationModel.from_pretrained("text-bison@001")
    tuned_model_names = model.list_tuned_model_names()
    print(tuned_model_names)
list_tuned_models(PROJECT_ID, REGION)
def fetch_model(project_id, location):

    aiplatform.init(project=project_id, location=location)
    model = TextGenerationModel.from_pretrained("text-bison@001")
    list_tuned_models = model.list_tuned_model_names()
    tuned_model = list_tuned_models[0]

    return tuned_model
deployed_model = fetch_model(PROJECT_ID, REGION)
deployed_model = TextGenerationModel.get_tuned_model(deployed_model)
PROMPT = """
How can I store my TensorFlow checkpoint on Google Cloud Storage?

Python example:

"""
print(deployed_model.predict(PROMPT))
# !pip install sequence-evaluate sentence-transformers --upgrade --user
from seq_eval import SeqEval
evaluator = SeqEval()
evaluation = evaluation.head(10) # you can change the number of rows you want to use
evaluation_question = evaluation["input_text"]
evaluation_answer = evaluation["output_text"]
candidates = []

for i in evaluation_question:
    response = deployed_model.predict(i)
    candidates.append(response.text)

len(candidates)
references = evaluation_answer.tolist()

len(references)
scores = evaluator.evaluate(candidates, references, verbose=False)
print(scores)

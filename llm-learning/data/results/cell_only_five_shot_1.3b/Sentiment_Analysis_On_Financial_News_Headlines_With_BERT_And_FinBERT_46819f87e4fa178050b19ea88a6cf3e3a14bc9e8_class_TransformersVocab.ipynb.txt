
    '''
    #@title Class TransformersVocab
    import torch
    from transformers import BertTokenizer, BertModel
    from transformers import AdamW
    from torch.utils.data import Dataset, DataLoader
    import torch.nn as nn
    import torch.optim as optim
    from tqdm import tqdm
    from sklearn.model_selection import train_test_split
    from sklearn.metrics import accuracy_score
    from transformers import BertForSequenceClassification

    class TransformersVocab(Dataset):
        def __init__(self, data, tokenizer, max_len):
            self.tokenizer = tokenizer
            self.data = data
            self.max_len = max_len

        def __len__(self):
            return len(self.data)

        def __getitem__(self, index):
            text = self.data[index]
            inputs = self.tokenizer.encode_plus(
                text,
                None,
                add_special_tokens=True,
                max_length=self.max_len,
                pad_to_max_length=True,
                return_token_type_ids=True
            )
            ids = inputs['input_ids']
            mask = inputs['attention_mask']

            return {
                'ids': torch.tensor(ids, dtype=torch.long),
                'mask': torch.tensor(mask, dtype=torch.long),
                'targets': torch.tensor(self.data[index], dtype=torch.long)
            }

    def train_epoch(model, data_loader, loss_fn, optimizer, device):
        model.train()
        losses = []
        for d in tqdm(data_loader, total=len(data_loader)):
            ids = d['ids'].to(device)
            mask = d['mask'].to(device)
            targets = d['targets'].to(device)

            outputs = model(ids, mask)
            _, preds = torch.max(outputs, dim=1)

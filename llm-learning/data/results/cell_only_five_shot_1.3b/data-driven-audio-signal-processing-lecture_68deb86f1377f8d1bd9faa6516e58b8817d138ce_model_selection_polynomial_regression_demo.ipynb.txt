
    ### Example 1: Polynomial Regression

    # Let's start with a simple example of polynomial regression. We will use the Boston Housing dataset, which is a regression problem.

    from sklearn.datasets import load_boston
    from sklearn.model_selection import train_test_split
    from sklearn.linear_model import LinearRegression
    from sklearn.preprocessing import PolynomialFeatures
    from sklearn import metrics

    # Load the Boston Housing dataset
    boston = load_boston()

    # Split the data into training/testing sets
    X_train, X_test, y_train, y_test = train_test_split(boston.data, boston.target, test_size=0.2, random_state=42)

    # Create Polynomial Features
    poly = PolynomialFeatures(2)

    # Transform the features to higher degree
    X_train_poly = poly.fit_transform(X_train)
    X_test_poly = poly.transform(X_test)

    # Fit the model
    model = LinearRegression()
    model.fit(X_train_poly, y_train)

    # Make predictions
    y_pred = model.predict(X_test_poly)

    # The coefficients
    print('Coefficients: \n', model.coef_)

    # The mean squared error
    print('Mean squared error: %.2f' % metrics.mean_squared_error(y_test, y_pred))
    print('Root Mean Squared Error: %.2f' % np.sqrt(metrics.mean_squared_error(y_test, y_pred)))
    print('R-squared: %.2f' % metrics.r2_score(y_test, y_pred))

    ### Example 2: Model Selection

    # Let's use cross-validation to select the best model. We will use the Ridge regression model.

    from sklearn.linear_model import Ridge
    from sklearn.model_selection import cross_val_score

    # Create
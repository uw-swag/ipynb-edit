```python
    # Import necessary libraries
    import torch
    import torch.nn as nn
    import torch.optim as optim

    # Define a simple neural network
    class Net(nn.Module):
        def __init__(self):
            super(Net, self).__init__()
            self.fc1 = nn.Linear(10, 5)
            self.fc2 = nn.Linear(5, 1)

        def forward(self, x):
            x = F.relu(self.fc1(x))
            x = self.fc2(x)
            return x

    # Create an instance of the network
    net = Net()

    # Define the loss function and the optimizer
    criterion = nn.MSELoss()
    optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)

    # Dummy input data
    input_data = torch.randn(10)

    # Forward pass
    output = net(input_data)

    # Compute the loss
    loss = criterion(output, torch.randn(1))

    # Backward pass and optimization step
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

    # Print the gradients
    print('Gradients:', net.fc1.weight.grad)
    ```
    This notebook demonstrates the creation of a simple neural network, defining a loss function and an optimizer, and then performing a forward pass, computing the loss, backpropagating the gradients, and updating the weights.

    Please note that this is a very basic example and real-world applications would require more complex architectures, more sophisticated loss functions, and more sophisticated optimizers.

```python
    import pandas as pd
    from sklearn.model_selection import train_test_split
    from sklearn.ensemble import RandomForestRegressor
    from sklearn.metrics import mean_squared_error

    # Load the data
    df = pd.read_csv('data.csv')

    # Split the data into features and target variable
    X = df.drop('index_date', axis=1)
    y = df['index_date']

    # Split the data into training and testing sets
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    # Train a Random Forest Regressor
    model = RandomForestRegressor(n_estimators=100, random_state=42)
    model.fit(X_train, y_train)

    # Make predictions on the testing set
    y_pred = model.predict(X_test)

    # Calculate the mean squared error of the predictions
    mse = mean_squared_error(y_test, y_pred)

    # Overfit the model by increasing the number of estimators
    model.set_params(n_estimators=1000)
    model.fit(X_train, y_train)

    # Make predictions on the testing set
    y_pred_overfit = model.predict(X_test)

    # Calculate the mean squared error of the overfit predictions
    mse_overfit = mean_squared_error(y_test, y_pred_overfit)

    print(f'Mean Squared Error (Original Model): {mse}')
    print(f'Mean Squared Error (Overfit Model): {mse_overfit}')
    ```

    ### Explanation:
    The code above is an example of overfitting in machine learning. Overfitting happens when a model learns the detail and noise in the training data to the extent that it negatively impacts the performance of the model on new data. In this case,
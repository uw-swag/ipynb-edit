
    Here is the refactored version of the code. I have refactored the `TrainingChapter4` class to a separate file `TrainingUtils.java`. I have also refactored the `plot-utils.ipynb` and `DataPoints.java` to a separate file `Utils.java`.

    Refactored `TrainingUtils.java`:
    ```java
    public class TrainingUtils {
        public static float epochLoss = 0f;
        public static float accuracyVal = 0f;

        public static void train(int numEpochs, int batchSize, float lr, IteratorManager manager, Network net, LossFunction loss) {
            for (int epoch = 1; epoch <= numEpochs; epoch++) {
                System.out.print("Running epoch " + epoch + "...... ");
                for (Batch batch : manager.getData(batchSize)) {
                    NDArray X = batch.getData().head();
                    NDArray y = batch.getLabels().head();

                    try (GradientCollector gc = Engine.getInstance().newGradientCollector()) {
                        NDArray yHat = net.forward(X, true); // net function call

                        NDArray lossValue = loss.evaluate(new NDList(y), new NDList(yHat));
                        NDArray l = lossValue.mul(batchSize);

                        epochLoss += l.sum().getFloat();

                        accuracyVal += TrainingUtils.accuracy(yHat, y);
                        gc.backward(l); // gradient calculation
                    }

                    batch.close();
                    TrainingUtils.sgd(net, lr, batchSize); // updater
                }

                float[] trainLoss = new float[epoch-1];
                float[] trainAccuracy = new float[epoch-1];
                float[] testAccuracy = new float[epoch-1];
                int[] epochCount = new int[epoch-1];

                epochLoss = 0f;
                accuracyVal = 0f;

                for (Batch batch : manager.getData(batchSize)) {
                    NDArray X
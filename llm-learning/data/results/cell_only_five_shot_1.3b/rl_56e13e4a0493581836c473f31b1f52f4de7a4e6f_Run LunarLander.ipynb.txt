```python
    import numpy as np
    import torch
    import torch.nn as nn
    import torch.optim as optim

    class ActorCritic(nn.Module):
        def __init__(self, state_dim, action_dim, max_action):
            super(ActorCritic, self).__init__()

            self.fc1 = nn.Linear(state_dim, 400)
            self.fc2_adv = nn.Linear(400, 200)
            self.fc3_adv = nn.Linear(200, action_dim)
            self.fc4_val = nn.Linear(400, 200)
            self.fc5_val = nn.Linear(200, 1)

            self.max_action = max_action

        def forward(self, state):
            a = torch.relu(self.fc1(state))
            a = torch.relu(self.fc2_adv(a))
            action = self.max_action * torch.tanh(self.fc3_adv(a))
            state_value = self.fc4_val(a)
            state_value = self.fc5_val(state_value)

            return action, state_value

    class ProximalPolicyOptimization:
        def __init__(self, state_dim, action_dim, max_action, lr, gamma, n_epochs):
            self.actor_critic = ActorCritic(state_dim, action_dim, max_action)
            self.lr = lr
            self.gamma = gamma
            self.n_epochs = n_epochs

            self.optimizer = optim.Adam(self.actor_critic.parameters(), lr=lr)

        def train(self, state, action, reward, next_state, done):
            state = torch.tensor(state, dtype=torch.float32)
            action = torch.tensor(action, dtype=torch.float32)
            reward = torch
```python
    import torch
    import torch.nn as nn

    class Seq2SeqAttention(nn.Module):
        def __init__(self, hidden_size, attention_size):
            super(Seq2SeqAttention, self).__init__()
            self.hidden_size = hidden_size
            self.attention_size = attention_size

            self.W1 = nn.Linear(self.hidden_size, self.attention_size)
            self.W2 = nn.Linear(self.hidden_size, self.attention_size)
            self.V = nn.Linear(self.attention_size, 1)

        def forward(self, hidden, encoder_outputs):
            # hidden: [batch_size, hidden_size]
            # encoder_outputs: [seq_len, batch_size, hidden_size]
            attn_weights = self.score(hidden, encoder_outputs)
            context_vector = self.vote(attn_weights)
            return context_vector, attn_weights

        def score(self, hidden, encoder_outputs):
            # hidden: [batch_size, hidden_size]
            # encoder_outputs: [seq_len, batch_size, hidden_size]
            attn_weights = torch.bmm(encoder_outputs, self.W1(hidden).unsqueeze(2))
            attn_weights = torch.nn.functional.softmax(attn_weights, dim=1)
            return attn_weights

        def vote(self, attn_weights):
            # attn_weights: [seq_len, batch_size]
            context_vector = torch.bmm(attn_weights.unsqueeze(2), encoder_outputs.transpose(0, 1))
            context_vector = context_vector.squeeze(2)
            context_vector = torch.nn.functional.relu(context_vector)
            return context_vector
    ```

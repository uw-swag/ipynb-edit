```python
    # Section 8.5: Implementation of Recurrent Neural Networks from Scratch (#88)
    # 8.5.1: Forward Propagation
    def forward_propagation(self, X):
        self.layers[0].A = X
        for layer_num in range(1, len(self.layers)):
            self.layers[layer_num].A = self.sigmoid(self.layers[layer_num].W.dot(self.layers[layer_num - 1].A) + self.layers[layer_num].b)
        return self.layers[-1].A

    # 8.5.2: Backward Propagation
    def backward_propagation(self, y, learning_rate):
        self.layers[-1].dA = self.y_pred - y
        for layer_num in range(len(self.layers) - 2, -1, -1):
            self.layers[layer_num].dW = self.layers[layer_num + 1].A.T.dot(self.layers[layer_num + 1].dA)
            self.layers[layer_num].db = np.sum(self.layers[layer_num + 1].dA, axis=0, keepdims=True)
            self.layers[layer_num].dA = self.layers[layer_num + 1].dA.dot(self.layers[layer_num].W.T)
        self.layers[0].dW = X.T.dot(self.layers[1].dA)
        self.layers[0].db = np.sum(self.layers[1].dA, axis=0, keepdims=True)

        # 8.5.3: Update Weights and Biases
        for layer_num in range(len(self.layers) - 1, -1, -1):
            self.layers[layer_num].W += self.layers[layer_num].dW * learning_rate
            self.layers[layer_num].b += self.layers[layer_num].db * learning_rate

    # 8.5.4: Train the R
```python
    import torch
    import torch.nn as nn
    import torch.nn.functional as F

    class NiN(nn.Module):
        def __init__(self, input_size, hidden_size, num_layers, output_size):
            super(NiN, self).__init__()
            self.input_size = input_size
            self.hidden_size = hidden_size
            self.num_layers = num_layers
            self.output_size = output_size

            self.nin_layers = []
            for i in range(num_layers):
                layer = nn.Linear(input_size if i == 0 else hidden_size, hidden_size)
                self.nin_layers.append(layer)
                input_size = hidden_size

            self.final_layer = nn.Linear(hidden_size, output_size)

        def forward(self, x):
            for i in range(self.num_layers):
                x = F.relu(self.nin_layers[i](x))
            x = self.final_layer(x)
            return F.log_softmax(x, dim=1)

    # Example usage:
    nin = NiN(input_size=10, hidden_size=5, num_layers=2, output_size=10)
    print(nin)
    ```

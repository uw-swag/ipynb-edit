```python
    import torch
from torchtext.legacy import data
from torchtext.legacy.data import Field, LabelField, TabularDataset, BucketIterator

# Define Fields
TEXT = Field(sequential=True, use_vocab=True, batch_first=True, pad_first=True)
LABEL = LabelField(dtype=torch.float)

# Define Dataset
train_data, test_data = TabularDataset.splits(path='your_data_path', train='train.csv', test='test.csv', format='csv', fields=[('text', TEXT), ('label', LABEL)])

# Build Vocabulary
TEXT.build_vocab(train_data, max_size=25000, vectors="glove.6B.100d", unk_init=torch.Tensor.normal_)

# Create Bucket Iterator
BATCH_SIZE = bs
device = torch.device(w)
train_iterator, valid_iterator, test_iterator = BucketIterator.splits((train_data, valid_data, test_data), batch_size=BATCH_SIZE, device=device)

# Define Model
class LSTM(torch.nn.Module):
    def __init__(self, input_dim, embedding_dim, hidden_dim, output_dim):
        super().__init__()
        self.embedding = torch.nn.Embedding(input_dim, embedding_dim)
        self.rnn = torch.nn.LSTM(embedding_dim, hidden_dim)
        self.fc = torch.nn.Linear(hidden_dim, output_dim)
        
    def forward(self, text):
        embedded = self.embedding(text)
        output, (hidden, cell) = self.rnn(embedded)
        assert torch.equal(output[-1,:,:], hidden.squeeze(0))
        return self.fc(hidden.squeeze(0))

# Initialize Model
model = LSTM(len(TEXT.vocab), 100, 
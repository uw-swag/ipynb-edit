```python
    # Importing necessary libraries
    import pythainlp
    from pythainlp.tokenize import word_tokenize
    from pythainlp.corpus import thai_sentiment

    # Loading the corpus
    corpus = thai_sentiment.sentences()

    # Tokenizing the sentences
    tokenized_sentences = [word_tokenize(sentence) for sentence in corpus]

    # Printing the tokenized sentences
    for i, tokenized_sentence in enumerate(tokenized_sentences):
        print(f"Sentence {i+1}: {tokenized_sentence}")

    # Importing necessary libraries
    import pythainlp
    from pythainlp.tokenize import word_tokenize
    from pythainlp.corpus import thai_wordnet

    # Loading the corpus
    corpus = thai_wordnet.words()

    # Tokenizing the words
    tokenized_words = [word_tokenize(word) for word in corpus]

    # Printing the tokenized words
    for i, tokenized_word in enumerate(tokenized_words):
        print(f"Word {i+1}: {tokenized_word}")

    # Importing necessary libraries
    import pythainlp
    from pythainlp.tokenize import char_tokenize
    from pythainlp.corpus import thai_wordnet

    # Loading the corpus
    corpus = thai_wordnet.words()

    # Tokenizing the words
    tokenized_words = [char_tokenize(word) for word in corpus]

    # Printing the tokenized words
    for i, tokenized_word in enumerate(tokenized_words):
        print(f"Word {i+1}: {tokenized_word}")
    ```

```python
    import numpy as np

    def softmax(x):
        e_x = np.exp(x - np.max(x))
        return e_x / e_x.sum(axis=0)

    def softmax_regression(X, y, learning_rate=0.01, epochs=1000):
        # Initialize weights randomly
        weights = np.random.randn(X.shape[1], 1)

        for epoch in range(epochs):
            # Forward pass
            z = np.dot(X, weights)
            predictions = softmax(z)

            # Compute loss
            loss = np.mean(-np.log(predictions[range(y.shape[0]), y]))

            # Backward pass
            gradients = np.dot(X.T, (predictions - y)) / y.shape[0]

            # Update weights
            weights -= learning_rate * gradients

        return weights

    # Example usage:
    X = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])
    y = np.array([0, 1, 2])
    weights = softmax_regression(X, y)
    print(weights)
    ```
    This code defines a function `softmax_regression` that implements softmax regression from scratch. The function takes as input the training data `X` and the target values `y`, and the learning rate and the number of epochs. It initializes the weights randomly, then iteratively updates the weights based on the gradient of the loss function. The function returns the final set of weights.

    The example usage at the end of the code demonstrates how to use the `softmax_regression` function to train a simple model on the example data.

```
    # Find path to PySpark
    import findspark
    findspark.init()

    # Import PySpark and initalize SparkContext object
    import pyspark
    sc = pyspark.SparkContext()

    # read the hamlet.txt file into an RDD (Resilient Distributed Data Set)
    raw_hamlet = sc.textFile('hamlet.txt')
    first_five_elements = raw_hamlet.take(5)
    print(first_five_elements)
    split_hamlet = raw_hamlet.map(lambda line: line.split('\t'))
    print(split_hamlet.collect())
    ```

    The code above reads the file 'hamlet.txt' and splits each line into words using the tab character as the delimiter. The `take(5)` function is used to get the first five lines of the RDD, and the `collect()` function is used to get the RDD as a list of strings.

    Please note that you need to have PySpark installed and the path to the 'hamlet.txt' file in your working directory.

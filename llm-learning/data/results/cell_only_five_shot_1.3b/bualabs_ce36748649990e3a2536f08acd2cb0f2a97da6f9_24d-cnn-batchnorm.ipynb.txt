```
    class BatchNorm(nn.Module):
    def __init__(self, nf, mom=0.1, eps=1e-5): 
        super().__init__()
        # mom is 1 - momentum
        self.mom, self.eps = mom, eps
        self.mults = nn.Parameter(torch.ones(nf, 1, 1))
        self.adds = nn.Parameter(torch.zeros(nf, 1, 1))
        self.register_buffer('vars', torch.ones(1, nf, 1, 1))
        self.register_buffer('means', torch.zeros(1, nf, 1, 1))

    def update_status(self, x):
        # find mean, var by batch, widht, height for each channel but keep dimension
        m = x.mean((0, 2, 3), keepdim=True)
        v = x.var((0, 2, 3), keepdim=True)
        self.means.lerp_(m, self.mom)
        self.vars.lerp_(v, self.mom)
        return m, v

    def forward(self, x):
        if self.training: 
            with torch.no_grad(): m, v = self.update_status(x)
        else: m, v = self.means, self.vars
        x = (x - m) / (v + self.eps).sqrt()
        return x * self.mults + self.adds

    def activation_chart(self, x):
        x = self.forward(x)
        return x
    ```

    This code adds an `activation_chart` method to the `BatchNorm` class. This method applies the batch normalization operation to the input `x` and returns the resulting activation. The activation is a visualization of the batch normalization process, showing the mean, variance, and standard deviation of the input data over time.

```
    # Tuning hyperparameters
    ## Hyperparameters:
    ##  - 'n_hid': number of hidden dimensions
    ##  - 'n_layers': number of layers in the encoder and decoder
    ##  - 'enc_emb_dropout': dropout applied to the embedding of the encoder
    ##  - 'dec_emb_dropout': dropout applied to the embedding of the decoder
    ##  - 'enc_out_dropout': dropout applied to the output of the encoder
    ##  - 'dec_out_dropout': dropout applied to the output of the decoder
    ##  - 'enc_emb_dropout_val': dropout applied to the embedding of the encoder during validation
    ##  - 'dec_emb_dropout_val': dropout applied to the embedding of the decoder during validation
    ##  - 'enc_out_dropout_val': dropout applied to the output of the encoder during validation
    ##  - 'dec_out_dropout_val': dropout applied to the output of the decoder during validation
    ##  - 'enc_dropout': dropout applied to the input of the encoder
    ##  - 'dec_dropout': dropout applied to the input of the decoder
    ##  - 'enc_emb_dropout_test': dropout applied to the embedding of the encoder during testing
    ##  - 'dec_emb_dropout_test': dropout applied to the embedding of the decoder during testing
    ##  - 'enc_out_dropout_test': dropout applied to the output of the encoder during testing
    ##  - 'dec_out_dropout_test': dropout applied to the output of the decoder during testing
    ##  - 'lr': learning rate
    ##  - 'clip': clip value for gradient clipping
    ##  - 'epochs': number of epochs
    ##  - 'max_grad_norm': maximum norm for gradients
    ##  - 'teacher_forcing': teacher forcing ratio
    ##  - 'grad_clip': clip value for gradients
    ##  - 'val_frac': fraction
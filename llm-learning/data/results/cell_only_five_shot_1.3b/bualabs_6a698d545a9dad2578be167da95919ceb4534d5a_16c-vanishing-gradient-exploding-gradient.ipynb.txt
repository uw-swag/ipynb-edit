```python
    import torch

    # Define a simple neural network with one hidden layer
    class Net(torch.nn.Module):
        def __init__(self):
            super(Net, self).__init__()
            self.fc1 = torch.nn.Linear(10, 100)  # input to hidden layer
            self.fc2 = torch.nn.Linear(100, 10)  # hidden to output layer

        def forward(self, x):
            x = torch.relu(self.fc1(x))  # apply ReLU activation function
            x = self.fc2(x)  # apply linear function
            return x

    # Initialize the network with Kaiming Initialization
    net = Net()
    for m in net.modules():
        if isinstance(m, torch.nn.Linear):
            torch.nn.init.kaiming_normal_(m.weight)
            if m.bias is not None:
                m.bias.data.zero_()
    ```
    This code initializes the weights of the network with Kaiming Initialization, which is a popular method for initializing the weights of neural networks. It does this by setting the weights to be randomly drawn from a normal distribution, and then scaling the values so that they have a standard deviation of sqrt(2 / fan_in) where fan_in is the number of input features.

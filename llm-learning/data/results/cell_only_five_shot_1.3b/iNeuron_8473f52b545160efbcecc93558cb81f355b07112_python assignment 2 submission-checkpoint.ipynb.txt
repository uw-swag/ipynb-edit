```
    # This code cell will not be shown in the HTML version of this notebook
    # run gradient descent to minimize the softmax cost
    g = cross_entropy; w = np.array([3.0,3.0])[:,np.newaxis]; max_its = 2000; alpha_choice = 1;
    weight_history,cost_history = optimizers.gradient_descent(g,alpha_choice,max_its,w)

    # create a static figure illustrating gradient descent steps 
    animator.static_fig(weight_history,num_contours = 25,viewmax = 12)

    # This code cell will not be shown in the HTML version of this notebook
    # run gradient descent to minimize the softmax cost
    g = cross_entropy; w = np.array([3.0,3.0])[:,np.newaxis]; max_its = 2000; alpha_choice = 1;
    weight_history,cost_history = optimizers.gradient_descent(g,alpha_choice,max_its,w)

    # create a static figure illustrating gradient descent steps 
    animator.static_fig(weight_history,num_contours = 25,viewmax = 12)

    # This code cell will not be shown in the HTML version of this notebook
    # run gradient descent to minimize the softmax cost
    g = cross_entropy; w = np.array([3.0,3.0])[:,np.newaxis]; max_its = 2000; alpha_choice = 1;
    weight_history,cost_history = optimizers.gradient_descent(g,alpha_choice,max_its,w)

    # create a static figure illustrating gradient descent steps 
    animator.static_fig(weight_history,num_contours = 25,viewmax = 12)

    # This code cell will not be shown in the HTML version of this notebook
    # run gradient descent to minimize the softmax cost
    g = cross_entropy; w = np.array([3.0,3.0])[:,np.newaxis]; max_its = 2000; alpha
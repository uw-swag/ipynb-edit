<Cell_0>
import pandas as pd
%matplotlib inline 
<\Cell_0>
<Cell_1>
df = pd.read_csv('./data/training_data.csv')
df.head(5)
<\Cell_1>
<Cell_2>
print(df['title'][1])
print('-----')
print(df['description'][1])
print('Published at:', df['publishedAt'][1])
<\Cell_2>
<Cell_3>
# Number of clickbait and non-clickbait articles
df['label'].value_counts()
<\Cell_3>
<Cell_4>
# Plotting the number of author fields that are Null
df['author'].isnull().value_counts().plot('barh')
<\Cell_4>
<Cell_5>
# The number of characters in the description field
df['description'].apply(len).mean()
<\Cell_5>
<Cell_6>
# Comparing the number of description characters in clickbait to news
df['description'].apply(len).groupby(df['label']).mean()
<\Cell_6>
<Cell_7>
# TEST YOUR KNOWLEDGE
# Can you write a one-liner to compute the number of clickbait articles
# written by each author? Hint: you might find the .sum() function helpful!
<\Cell_7>
<Cell_8>
df['full_content'] = (df.description + df.title)
df.head(1)
<\Cell_8>
<Cell_9>
from sklearn.feature_extraction.text import CountVectorizer
vectorizer = CountVectorizer()
<\Cell_9>
<Cell_10>
sentence = ["Literally just 8 really really cute dogs"]
vectorizer.fit(sentence)
print(vectorizer.vocabulary_) # dictionary of words and ids
<\Cell_10>
<Cell_11>
vectorizer.transform(sentence).toarray()
<\Cell_11>
<Cell_12>
sentence = ["OMG 5 truly hilarious dogs ðŸ˜‚"]
vectorizer.transform(sentence).toarray()
<\Cell_12>
<Cell_13>
from sklearn.svm import LinearSVC
svc = LinearSVC()
<\Cell_13>
<Cell_14>
bag_of_words = [
    [1, 5], [1, 4], [2, 6], [4,2], [3,4], [2, 1]
]
labels = [1, 1, 1, 0, 0, 0]
<\Cell_14>
<Cell_15>
from workshop import plot_2d_samples
plot_2d_samples(bag_of_words, labels)
<\Cell_15>
<Cell_16>
svc = svc.fit(bag_of_words, labels)
<\Cell_16>
<Cell_17>
from workshop import plot_2d_trained_svc
plot_2d_trained_svc(bag_of_words, labels, svc)
<\Cell_17>
<Cell_18>
svc.predict([[3, 1], [2,4]])
<\Cell_18>
<Cell_19>
steps = (
    ('vectorizer', CountVectorizer()),
    ('classifier', LinearSVC())
)
<\Cell_19>
<Cell_20>
from sklearn.pipeline import Pipeline
pipeline = Pipeline(steps)
<\Cell_20>
<Cell_21>
from sklearn.model_selection import train_test_split
training, testing = train_test_split(
    df,                # The dataset we want to split
    train_size=0.7,    # The proportinal size of our training set
    stratify=df.label, # The labels are used for stratification
    random_state=400   # Use the same random state for reproducibility
)
<\Cell_21>
<Cell_22>
training.head(5)
<\Cell_22>
<Cell_23>
print(len(training))
print(len(testing))
<\Cell_23>
<Cell_24>
pipeline = pipeline.fit(training.full_content, training.label)
<\Cell_24>
<Cell_25>
pipeline.predict(["Literally just 8 incredible dog videos."])
<\Cell_25>
<Cell_26>
pipeline.predict(["French election polls show an early lead for Macron."])
<\Cell_26>
<Cell_27>
steps = (
    ('vectorizer', CountVectorizer()),
    ('classifier', LinearSVC())
)
pipeline = Pipeline(steps)
<\Cell_27>
<Cell_28>
gs_params = {
    'vectorizer__stop_words': ['english', None],
    'vectorizer__ngram_range': [(1, 1), (1, 2), (2, 2)],
    'classifier__C': [0.001, 0.01, 0.1, 1, 10, 100]
}
<\Cell_28>
<Cell_29>
from sklearn.model_selection import GridSearchCV
gs = GridSearchCV(pipeline, gs_params, n_jobs=-1)
gs.fit(training.full_content, training.label)
<\Cell_29>
<Cell_30>
print(gs.best_params_)
<\Cell_30>
<Cell_31>
pipeline = gs.best_estimator_
<\Cell_31>
<Cell_32>
predicted_label = pipeline.predict(testing.full_content)
<\Cell_32>
<Cell_33>
from sklearn.metrics import accuracy_score
print("Accuracy = {:.1f}%".format(
    accuracy_score(testing.label, predicted_label) * 100.0
))
<\Cell_33>
<Cell_34>
from sklearn.metrics import confusion_matrix
from workshop import plot_confusion_matrix

cm = confusion_matrix(testing.label, predicted_label)
plot_confusion_matrix(cm, pipeline.classes_)
<\Cell_34>
<Cell_35>
filename = 'classifiers/clickbait_svc_v1'
<\Cell_35>
<Cell_36>
import pickle
with open(filename, 'wb') as f:
    pickle.dump(pipeline, f)
<\Cell_36>
<Cell_37>

<\Cell_37>

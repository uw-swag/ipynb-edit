<Cell_0>
import numpy as np
y_pred = np.array([0.000, 0.100, 0.200])
y_true = np.array([0.000, 0.200, 0.250])
# Defining Mean Absolute Error loss function
def mae(pred, true):
    # Find absolute difference
    differences = pred - true
    absolute_differences = np.absolute(differences)
    # find the absoute mean
    mean_absolute_error = absolute_differences.mean()
    return mean_absolute_error
mae_value = mae(y_pred, y_true)
print ("MAE error is: " + str(mae_value))
<\Cell_0>
<Cell_1>
mae_loss = nn.L1Loss()
<\Cell_1>
<Cell_2>
import torch
mae_loss = torch.nn.L1Loss()
input = torch.tensor(y_pred)
target = torch.tensor(y_true)
output = mae_loss(input, target)
print(output)
<\Cell_2>
<Cell_3>
input = torch.randn(3, 4, requires_grad=True)
target = torch.randn(3, 4)

mse_loss = nn.MSELoss()

output = mse_loss(input, target)
output.backward()

print('input -: ', input)
print('target -: ', target)
print('output -: ', output)
<\Cell_3>
<Cell_4>
import numpy as np
y_pred = np.array([0.1580, 0.4137, 0.2285])
y_true = np.array([0.0, 1.0, 0.0]) #2 labels: (0,1)
def BCE(y_pred, y_true):
    total_bce_loss = np.sum(-y_true * np.log(y_pred) - (1 - y_true) * np.log(1 - y_pred))
    # Getting the mean BCE loss
    num_of_samples = y_pred.shape[0]
    mean_bce_loss = total_bce_loss / num_of_samples
    
    return mean_bce_loss

bce_value = BCE(y_pred, y_true)
print ("BCE error is: " + str(bce_value))
<\Cell_4>
<Cell_5>
bce_loss = torch.nn.BCELoss()
sigmoid = torch.nn.Sigmoid() # Ensuring inputs are between 0 and 1
input = torch.tensor(y_pred)
target = torch.tensor(y_true)
output = bce_loss(input, target)
output
<\Cell_5>
<Cell_6>
input = torch.randn(3, 5, requires_grad=True)
# every element in target should have value(0 <= value < C)
target = torch.tensor([1, 0, 4])

m = nn.LogSoftmax(dim=1)
nll_loss = nn.NLLLoss()
output = nll_loss(m(input), target)
output.backward()

print('input -: ', input)
print('target -: ', target)
print('output -: ', output)
<\Cell_6>
<Cell_7>
input = torch.randn(3, 5, requires_grad=True)
target = torch.empty(3, dtype=torch.long).random_(5)

cross_entropy_loss = nn.CrossEntropyLoss()
output = cross_entropy_loss(input, target)
output.backward()

print('input: ', input)
print('target: ', target)
print('output: ', output)
<\Cell_7>
<Cell_8>
input = torch.randn(3, 5, requires_grad=True)
target = torch.randn(3, 5)

hinge_loss = nn.HingeEmbeddingLoss()
output = hinge_loss(input, target)
output.backward()

print('input -: ', input)
print('target -: ', target)
print('output -: ', output)
<\Cell_8>
<Cell_9>
first_input = torch.randn(3, requires_grad=True)
Second_input = torch.randn(3, requires_grad=True)
target = torch.randn(3).sign()

ranking_loss = nn.MarginRankingLoss()
output = ranking_loss(first_input, Second_input, target)
output.backward()

print('input one: ', first_input)
print('input two: ', Second_input)
print('target: ', target)
print('output: ', output)
<\Cell_9>
<Cell_10>
anchor = torch.randn(100, 128, requires_grad=True)
positive = torch.randn(100, 128, requires_grad=True)
negative = torch.randn(100, 128, requires_grad=True)

triplet_margin_loss = nn.TripletMarginLoss(margin=1.0, p=2)
output = triplet_margin_loss(anchor, positive, negative)
output.backward()

print('anchors -: ', anchor)
print('positive -: ', positive)
print('negative -: ', negative)
print('output -: ', output)
<\Cell_10>
<Cell_11>
input = torch.randn(2, 3, requires_grad=True)
target = torch.randn(2, 3)

kld_loss = nn.KLDivLoss(reduction = 'batchmean')
output = kld_loss(input, target)
output.backward()

print('input tensor: ', input)
print('target tensor: ', target)
print('Loss: ', output)
<\Cell_11>
<Cell_12>

<\Cell_12>

<Cell_0>
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.ensemble import RandomForestClassifier
from sklearn import svm
from sklearn.neural_network import MLPClassifier
from sklearn.metrics import confusion_matrix, classification_report
from sklearn.metrics import accuracy_score
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.model_selection import train_test_split
%matplotlib inline
<\Cell_0>
<Cell_1>
#load the dataset
wine = pd.read_csv('winequality-red.csv',sep=';')
<\Cell_1>
<Cell_2>
wine.head()
<\Cell_2>
<Cell_3>
wine.info()
<\Cell_3>
<Cell_4>
#Preprocess data
bins = (2, 6.5, 8)
group_names = ['bad','good']
wine['quality'] = pd.cut(wine['quality'],bins = bins, labels = group_names)
wine['quality'].unique()
<\Cell_4>
<Cell_5>
label_quality = LabelEncoder()
<\Cell_5>
<Cell_6>
wine['quality'] = label_quality.fit_transform(wine['quality'])
<\Cell_6>
<Cell_7>
wine.head(10)
<\Cell_7>
<Cell_8>
wine['quality'].value_counts()
<\Cell_8>
<Cell_9>
sns.countplot(wine['quality'])
<\Cell_9>
<Cell_10>
#Seperate the dataset as response variable and feature variables
X = wine.drop('quality', axis = 1)
y = wine['quality']
<\Cell_10>
<Cell_11>
#Train and test the splitting of data
X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2,random_state=42)
<\Cell_11>
<Cell_12>
#Applying standard scalling to get an optimum result
sc = StandardScaler()
X_train = sc.fit_transform(X_train)
X_test = sc.transform(X_test)
<\Cell_12>
<Cell_13>
X_train[:10]
<\Cell_13>
<Cell_14>
rfc = RandomForestClassifier(n_estimators=200)
rfc.fit(X_train,y_train)
pred_rfc=rfc.predict(X_test)
<\Cell_14>
<Cell_15>
#Testing the model
print(classification_report(y_test,pred_rfc))
print(confusion_matrix(y_test,pred_rfc))
<\Cell_15>
<Cell_16>
clf=svm.SVC()
clf.fit(X_train,y_train)
pred_clf=clf.predict(X_test)
<\Cell_16>
<Cell_17>
#Testing the model
print(classification_report(y_test,pred_clf))
print(confusion_matrix(y_test,pred_clf))
<\Cell_17>
<Cell_18>
mlpc=MLPClassifier(hidden_layer_sizes=(11,11,11),max_iter=500)
mlpc.fit(X_train,y_train)
pred_mlpc=mlpc.predict(X_test)
<\Cell_18>
<Cell_19>
#Testing the model
print(classification_report(y_test,pred_mlpc))
print(confusion_matrix(y_test,pred_mlpc))
<\Cell_19>
<Cell_20>
cm = accuracy_score(y_test, pred_rfc)
cm
<\Cell_20>
<Cell_21>
wine.head(10)
<\Cell_21>
<Cell_22>
#Test the Random Forest Classifier (the best) on a new wine
Xnew = [[7.3,0.58,0.00,2.0,0.065,15.0,21.0,0.9946,3.36,0.47,10.0]]
Xnew = sc.transform(Xnew)
ynew = rfc.predict(Xnew)
ynew
<\Cell_22>
<Cell_23>
r_probs = [0 for _ in range(len(y_test))]
rfc_probs = ynew = rfc.predict_proba(X_test)
mlpc_probs = pred_mlpc=mlpc.predict_proba(X_test)
<\Cell_23>
<Cell_24>
from sklearn.metrics import roc_curve, roc_auc_score
r_auc = roc_auc_score(y_test, r_probs)
rfc_auc = roc_auc_score(y_test, rfc_probs)
mlpc_auc = roc_auc_score(y_test, mlpc_probs)
<\Cell_24>

<Cell_0>
import pandas as pd
import numpy as np

from datetime import datetime
from datetime import time
from datetime import timedelta

from sklearn import datasets, ensemble, model_selection
<\Cell_0>
<Cell_1>
from evidently.ui.workspace.cloud import CloudWorkspace

from evidently import ColumnMapping
from evidently.report import Report

from evidently.metrics import ColumnSummaryMetric, ColumnDistributionMetric, ColumnDriftMetric, DataDriftTable, TextDescriptorsDistribution

from evidently.metric_preset import DataDriftPreset, DataQualityPreset, TextOverviewPreset

from evidently.descriptors import HuggingFaceModel, OpenAIPrompting 
from evidently.descriptors import RegExp, BeginsWith, EndsWith, Contains, DoesNotContain, IncludesWords, ExcludesWords
from evidently.descriptors import TextLength, OOV, NonLetterCharacterPercentage, SentenceCount, WordCount, Sentiment
<\Cell_1>
<Cell_2>
import nltk
nltk.download('words')
nltk.download('wordnet')
nltk.download('omw-1.4')
nltk.download('vader_lexicon')
<\Cell_2>
<Cell_3>
assistant_logs =  pd.read_csv('chat_df.csv', index_col=0, parse_dates=['start_time', 'end_time'])
assistant_logs.index = assistant_logs.start_time
assistant_logs.index.rename('index', inplace=True)
<\Cell_3>
<Cell_4>
assistant_logs[["question", "response"]].head()
<\Cell_4>
<Cell_5>
assistant_logs.iloc[6].question
<\Cell_5>
<Cell_6>
assistant_logs.iloc[6].response
<\Cell_6>
<Cell_7>
column_mapping = ColumnMapping(
    datetime='start_time',
    datetime_features=['end_time'],
    text_features=['question', 'response'],
    categorical_features=['organization', 'model_ID', 'region', 'environment', 'feedback'],
)
<\Cell_7>
<Cell_8>
#Built-in descriptors without parameters
report = Report(metrics=[
        ColumnSummaryMetric(column_name = Sentiment(display_name="Question sentiment").for_column("question")),
        ColumnSummaryMetric(column_name = TextLength(display_name= "Question length").for_column("question")),
        ColumnSummaryMetric(column_name = OOV(display_name= "Question out of vocabulary words").for_column("question")),
        ColumnSummaryMetric(column_name = Sentiment(display_name="Response sentiment").for_column("response")),
        ColumnSummaryMetric(column_name = NonLetterCharacterPercentage(display_name="Non letter characters in response").for_column("response")),
        ColumnSummaryMetric(column_name = SentenceCount(display_name="Sentence count in response").for_column("response")),
        ColumnSummaryMetric(column_name = WordCount(display_name="Word count in response").for_column("response")),
])

report.run(reference_data=assistant_logs[datetime(2024, 4, 8) : datetime(2024, 4, 9)], 
           current_data=assistant_logs[datetime(2024, 4, 9) : datetime(2024, 4, 10)], 
           column_mapping=column_mapping)
report    
<\Cell_8>
<Cell_9>
#Built-in descriptors with parameters
report = Report(metrics=[
        ColumnSummaryMetric(column_name = BeginsWith(display_name="'How' question", prefix="How").for_column("question")),
        ColumnSummaryMetric(column_name = EndsWith(display_name="Assisrance might be needed", suffix="for assistance.").for_column("response")),
        ColumnSummaryMetric(column_name = RegExp(reg_exp=r"^I", display_name= "Question begins with 'I'").for_column("question")), 
        ColumnSummaryMetric(column_name = IncludesWords(words_list=['invoice', 'salary'],
                                                               display_name="Questions about invoices and salary").for_column("question")),
        ColumnSummaryMetric(column_name = ExcludesWords(words_list=['wrong', 'mistake'], 
                                                               display_name="Responses without mention of mistakes").for_column("response")),
        ColumnSummaryMetric(column_name = Contains(items=['medical leave'], 
                                                               display_name="contains 'medical leave'").for_column("response")),
        ColumnSummaryMetric(column_name = DoesNotContain(items=['employee portal'], 
                                                               display_name="does not contain 'employee portal'").for_column("response")),
])

report.run(reference_data=assistant_logs[datetime(2024, 4, 8) : datetime(2024, 4, 9)], 
           current_data=assistant_logs[datetime(2024, 4, 9) : datetime(2024, 4, 10)], 
           column_mapping=column_mapping)
report    
<\Cell_9>
<Cell_10>
pii_prompt = """
Personally identifiable information (PII) is information that, when used alone or with other relevant data, can identify an individual.

PII may contain direct identifiers (e.g., passport information) that can identify a person uniquely, 
or quasi-identifiers (e.g., race) that can be combined with other quasi-identifiers (e.g., date of birth) to successfully recognize an individual.
PII may contain person's name, person's address,and something I may forget to mention

Please identify whether or not the above text contains PII

text: REPLACE 

Use the following categories for PII identification:
1 if text contains PII
0 if text does not contain PII
0 if the information provided is not sufficient to make a clear determination

Retrun a category only
"""

negativity_prompt = """
Classify text into two groups: negative and positive

text: REPLACE 

Use the following categories for classification:
NEGATIVE if text is negative
POSITIVE if text is NOT negative
UNKNOWN use this category only if the information provided is not sufficient to make a clear determination

Retrun only category
"""
<\Cell_10>
<Cell_11>
#Descriptors with external models
#to run OpenAIPrompting descriptor make sure you set environement variable with openai token 
report = Report(metrics=[
        ColumnSummaryMetric(column_name = HuggingFaceModel("toxicity", "DaNLP/da-electra-hatespeech-detection", {"module_type": "measurement"}, {"toxic_label": "offensive"}, "toxicity", display_name="Hugging Face Toxicity for response").for_column("response")),
        ColumnSummaryMetric(column_name = OpenAIPrompting(prompt=pii_prompt, 
                                                      prompt_replace_string="REPLACE", 
                                                      model="gpt-3.5-turbo-instruct", 
                                                      feature_type="num",
                                                      display_name="PII for response (by gpt3.5)").for_column("response")),
        ColumnSummaryMetric(column_name = OpenAIPrompting(prompt=negativity_prompt, 
                                                      prompt_replace_string="REPLACE", 
                                                      model="gpt-3.5-turbo-instruct", 
                                                      feature_type="cat",
                                                      display_name="Negativity for response (by gpt3.5)").for_column("response")),
])

report.run(reference_data=assistant_logs[datetime(2024, 4, 8) : datetime(2024, 4, 9)], 
           current_data=assistant_logs[datetime(2024, 4, 9) : datetime(2024, 4, 10)], 
           column_mapping=column_mapping)

report    
<\Cell_11>
<Cell_12>
#reference dataset enriched with descriptors
report.datasets()[0]
<\Cell_12>
<Cell_13>
#current dataset enriched with descriptors
report.datasets()[1]
<\Cell_13>

<Cell_0>
import os
from os.path import join as oj
import sys, time
sys.path.insert(1, oj(sys.path[0], '..'))  # insert parent path
import numpy as np
import matplotlib.pyplot as plt
from tqdm import tqdm
from copy import deepcopy
import pickle as pkl
import pandas as pd
from os.path import join
import torch
import torch
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
import seaborn as sns

%matplotlib inline
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from os.path import join as oj
import torch.utils.data as utils
from torchvision import datasets, transforms
import numpy as np
import os
import sys
pd.set_option('precision', 3)
<\Cell_0>
<Cell_1>

<\Cell_1>
<Cell_2>

<\Cell_2>
<Cell_3>

<\Cell_3>
<Cell_4>
save_path = "../results_for_export"
trained_model_folder = '../models/MNIST'
fnames = [oj(trained_model_folder, fname) for fname in os.listdir(trained_model_folder)]
# other models were trained badly

results_list = [pd.Series(pkl.load(open(fname, "rb"))) for fname in (fnames)] 
results = pd.concat(results_list, axis=1).T.infer_objects() 
results['final_acc'] = [max(x) for x in results['accs_test']] 
results = results[results.method!= "ExpectedGrad"]
<\Cell_4>
<Cell_5>
results['final_acc'] = [max(x) for x in results['accs_test']] 
results['final_acc_train'] = [max(x) for x in results['accs_train']]
results['final_cd'] = [x[-1] for x in results['cd']]
results['final_test_loss'] = [min(x) for x in results['losses_test']]
results['final_train_loss'] = [min(x) for x in results['losses_train']]
results['acc_color'] = [0 for x in results['accs_train']]


# results = results.dropna()
<\Cell_5>
<Cell_6>
results = results[results.dataset == "Color"]
<\Cell_6>
<Cell_7>


results.reset_index(drop=True, inplace=True)
results_save = results[['regularizer_rate','final_acc','final_acc_train','final_test_loss', 'acc_color','final_cd', 'method', 'dataset']].sort_values(by = ['regularizer_rate'])
<\Cell_7>
<Cell_8>
concise_results = results_save.groupby(by = ['dataset', 'method','regularizer_rate']).mean()
concise_results.columns = ['Test accuracy', 'Train accuracy', 'Test loss', 'Color accuracy', 'CD']
# with open(oj(save_path, "color_mnist.text"), 'w') as f:
#           f.write(concise_results.to_latex())
<\Cell_8>
<Cell_9>
concise_results
<\Cell_9>
<Cell_10>
concise_results_for_save = concise_results[['Test accuracy']]#, 'Color accuracy']]
<\Cell_10>
<Cell_11>
concise_results_for_save
<\Cell_11>
<Cell_12>
class Net(nn.Module):
    def __init__(self):
        super(Net, self).__init__()
        self.conv1 = nn.Conv2d(3, 20, 5, 1)
        self.conv2 = nn.Conv2d(20, 50, 5, 1)
        self.conv2_drop = nn.Dropout2d()

        self.fc1 = nn.Linear(4*4*50, 500)
        self.fc2 = nn.Linear(500, 10)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        x = F.max_pool2d(x, 2, 2)
        x = F.relu(self.conv2(x))
        x = F.max_pool2d(x, 2, 2)
        x = x.view(-1, 4*4*50)
        
        x = F.dropout(x, training=self.training)
        x = F.relu(self.fc1(x))
        
        x = self.fc2(x)
        return F.log_softmax(x, dim=1)
    def logits(self, x):
    
        x = F.relu(self.conv1(x))
        x = F.max_pool2d(x, 2, 2)
        x = F.relu(self.conv2(x))
        x = F.max_pool2d(x, 2, 2)
        x = x.view(-1, 4*4*50)
        x = F.dropout(x, training=self.training)
        x = F.relu(self.fc1(x))
        
        x = self.fc2(x)
        return x

<\Cell_12>
<Cell_13>
def test( model, device, test_loader, epoch):
    model.eval()
    test_loss = 0
    correct = 0
    with torch.no_grad():
        for data, target in test_loader:
            data, target = data.to(device), target.to(device)
            output = model(data)
            test_loss += F.nll_loss(output, target, reduction='sum').item() # sum up batch loss
            pred = output.argmax(dim=1, keepdim=True) # get the index of the max log-probability
            correct += pred.eq(target.view_as(pred)).sum().item()

    test_loss /= len(test_loader.dataset)
    return(test_loss, 100.*correct / len(test_loader.dataset))

<\Cell_13>
<Cell_14>
kwargs = {'num_workers': 1, 'pin_memory': True}
test_x_tensor = torch.Tensor(np.load(oj("../data/ColorMNIST", "test_x.npy")))
test_y_color= torch.Tensor(np.load(oj("../data/ColorMNIST", "test_y.npy"))).type(torch.int64)
test_dataset_color = utils.TensorDataset(test_x_tensor,test_y_color) # create your datset

test_loader_color = utils.DataLoader(test_dataset_color,
        batch_size=256, shuffle=True, **kwargs) # create your dataloader

test_net = Net()
test_net = test_net.to(0)
<\Cell_14>
<Cell_15>
acc_color_list = []
loss_color_list = []
for i in range(len(results)):
    test_net.load_state_dict(results.model_weights[i])
    loss_col, acc_col = test(test_net, 0, test_loader_color, 0)
    acc_color_list.append(acc_col)
    loss_color_list.append(loss_col)
results["final_acc"] =[x for x in acc_color_list]
results["final_test_loss"] =[x for x in loss_color_list]               
               

<\Cell_15>
<Cell_16>
kwargs = {'num_workers': 1, 'pin_memory': True}
test_x_tensor = torch.Tensor(np.load(oj("../data/ColorMNIST", "test_x.npy")))
test_y_color= torch.Tensor(np.load(oj("../data/ColorMNIST", "test_y.npy"))).type(torch.int64)
test_dataset_color = utils.TensorDataset(test_x_tensor,test_y_color) # create your datset

test_loader_color = utils.DataLoader(test_dataset_color,
        batch_size=256, shuffle=True, **kwargs) # create your dataloader

test_net = Net()
test_net = test_net.to(0)
<\Cell_16>
<Cell_17>
acc_color_list = []
loss_color_list = []
for i in tqdm(range(len(results))):
    test_net.load_state_dict(results.model_weights[i])
    loss_col, acc_col = test(test_net, 0, test_loader_color, 0)
    acc_color_list.append(acc_col)
    loss_color_list.append(loss_col)
results["acc_color"] =[x for x in acc_color_list]
results["loss_color"] =[x for x in loss_color_list]               
               

<\Cell_17>
<Cell_18>
pd.set_option('precision', 2)
<\Cell_18>
<Cell_19>
plot_results = results[['method', 'regularizer_rate',"final_acc"]].groupby(by = ['method','regularizer_rate']).mean()
<\Cell_19>
<Cell_20>
results = results[results.regularizer_rate < 5000]
<\Cell_20>
<Cell_21>
sns.set()
<\Cell_21>
<Cell_22>
sns.reset_defaults()
<\Cell_22>
<Cell_23>
plot = sns.lineplot(x ='regularizer_rate' , y='final_acc', data = results[results.method == 'CDEP'],label="CDEP",ci=None ,legend = "brief"  );
plot = sns.lineplot(x ='regularizer_rate' , y='final_acc', data = results[results.method == 'Grad'], label="RRR" ,ci=None );
plot = sns.lineplot(x ='regularizer_rate' , y='final_acc', data = results[results.method == 'EGradients'], label="Expected Gradients",ci=None  );
plot.set(xscale="log")
#plot.set(yscale="log")
plot.set_xlabel("Regularization strength")
plot.set_ylabel("Test Accuracy")
plot.set_title("");
fig  = plot.get_figure()
fig.tight_layout()
fig.savefig(oj(save_path,"ColorMNIST_results"))
<\Cell_23>
<Cell_24>

<\Cell_24>

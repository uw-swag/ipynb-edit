<Cell_0>
import pickle as pckl
<\Cell_0>
<Cell_1>
import os
<\Cell_1>
<Cell_2>
folder = "../models/trained_models"
<\Cell_2>
<Cell_3>
pckl_list = os.listdir("../models/trained_models")
<\Cell_3>
<Cell_4>
pckl_list
<\Cell_4>
<Cell_5>
with open(os.path.join(folder, pckl_list[-1]), 'rb') as f:
    test  = pckl.load(f)
<\Cell_5>
<Cell_6>
test
<\Cell_6>
<Cell_7>
%load_ext autoreload
%autoreload 2
%matplotlib inline
import numpy as np
import matplotlib.pyplot as plt
import torch
import pickle
import sys
import os
sys.path.append('../../acd/acd')
sys.path.append('../../acd/visualization')
sys.path.append('../../acd/acd/util')
sys.path.append('../../acd/acd/scores')
sys.path.append('../../acd/acd/agglomeration')
import viz_1d as viz
import tiling_1d as tiling
import agg_1d as agg
import cd
import score_funcs
sys.path.append('../../acd')
sys.path.append('../../acd/dsets/sst')
from dsets.sst import dset
from dsets.sst.model import LSTMSentiment

# check out how two models differ
import torch.optim as O
import torch.nn as nn

from torchtext import data
from torchtext import datasets
from os.path import join
<\Cell_7>
<Cell_8>

vector_cache =  '../data/.vector_cache/input_vectors.pt'
word_vectors ='glove.6B.300d'
batch_size = 50
device = 'cuda' if torch.cuda.is_available() else 'cpu'
<\Cell_8>
<Cell_9>

inputs = data.Field(lower= True)
answers = data.Field(sequential=False, unk_token=None)

train, dev, test = datasets.SST.splits(inputs, answers, fine_grained=False, train_subtrees=True,
                                       filter_pred=lambda ex: ex.label != 'neutral')

inputs.build_vocab(train, dev, test)

if os.path.isfile(vector_cache):
    inputs.vocab.vectors = torch.load(vector_cache)
else:
    inputs.vocab.load_vectors(word_vectors)
    makedirs(os.path.dirname(vector_cache))
    torch.save(inputs.vocab.vectors,vector_cache)
answers.build_vocab(train)

train_iter, dev_iter, test_iter = data.BucketIterator.splits(
    (train, dev, test), batch_size=batch_size, device=torch.device(0))
<\Cell_9>
<Cell_10>
model_path = "../models/init_models"
model_list = os.listdir(model_path)
model1 = torch.load(join(model_path, model_list[0]), map_location=torch.device(0)).eval()

model2 = torch.load(join(model_path, model_list[1]), map_location=torch.device(0)).eval()
<\Cell_10>
<Cell_11>

is_correct = np.empty((2,len(dev)))
<\Cell_11>
<Cell_12>
with torch.no_grad():
    # check out how two models differ
    import torch.optim as O
    import torch.nn as nn
    criterion = nn.CrossEntropyLoss()
    n_dev_correct, dev_loss = 0, 0
    cur_idx = 0
    for dev_batch_idx, dev_batch in enumerate(dev_iter):



        answer1 = model1(dev_batch)
        answer2  = model2(dev_batch)

        is_correct[0, cur_idx:cur_idx + dev_batch.label.shape[0]] = (torch.max(answer1, 1)[1].view(dev_batch.label.size()).data == dev_batch.label.data).cpu().numpy()
        is_correct[1, cur_idx:cur_idx + dev_batch.label.shape[0]] = (torch.max(answer2, 1)[1].view(dev_batch.label.size()).data == dev_batch.label.data).cpu().numpy()

        cur_idx+=dev_batch.label.shape[0]
        n_dev_correct += (((torch.max(answer1, 1)[1].view(dev_batch.label.size()).data == dev_batch.label.data)+
        (torch.max(answer2, 1)[1].view(dev_batch.label.size()).data == dev_batch.label.data))>=1).sum()
        dev_loss = criterion(answer1, dev_batch.label)
    dev_acc = 100. * n_dev_correct / len(dev)
<\Cell_12>
<Cell_13>
is_correct -= is_correct.mean(axis=1)[:,np.newaxis]
<\Cell_13>
<Cell_14>
is_correct /=(is_correct.std(axis=1)[:,np.newaxis])
<\Cell_14>
<Cell_15>
np.cov(is_correct[0], is_correct[1])
<\Cell_15>
<Cell_16>

<\Cell_16>

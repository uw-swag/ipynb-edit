<Cell_0>
#read/write image data
!pip install imageio
#deep learning library

!pip3 install http://download.pytorch.org/whl/cu80/torch-0.3.0.post4-cp36-cp36m-linux_x86_64.whl
!pip3 install torchvision
 #access kaggle datasets from colab
!pip install kaggle
#model loading
!pip install ipywidgets
!pip install --no-cache-dir -I pillow
<\Cell_0>
<Cell_1>
#file input ouput
import os
#matrix math
import numpy as np
#read/write image data
import imageio
#visualize data
import matplotlib.pyplot as plt
#data preprocessing
import pandas as pd
#deep learninig
import torch
#just in case we need a backup datasets
from torch.utils import data
#willl ouput the plot right below the cell that produces it
%matplotlib inline

<\Cell_1>
<Cell_2>
#allows us to upload files into coalb
# we'll need to upload the kaggle.jason file
#in kaggle, under accounts, click 'create new API token'
#upload the kaggle.jason file that is automatically downloaded
from google.colab import files
files.upload()
<\Cell_2>
<Cell_3>
#ensure its there
!ls -lha kaggle.json

<\Cell_3>
<Cell_4>
#the Kaggle API client expect this file to be in ~/.kaggle,
# so lets move it there
!mkdir -p ~/.kaggle
!cp kaggle.json ~/.kaggle/

#this permissions change avoids a warning on kaggle tool startup.
!chmod 600  ~/.kaggle/kaggle.json

<\Cell_4>
<Cell_5>
# lets now download our dataset
!kaggle competitions download -c tgs-salt-identification-challenge
<\Cell_5>
<Cell_6>
#and we'll need those training images unzipped
!ls
!unzip train.zip

<\Cell_6>
<Cell_7>
#lets create a class to represent this data , to  make it easier to access

class TGSSaltDataset(data.Dataset):
  #initwith the location of the dataset, and the list of file 
  def __init__(self, root_path, file_list):
    self.root_path = root_path
    self.file_list = file_list
    
   #get method - how long is the list
  def __len__(self):
    return len(self.file_list)
  #get  method - return the seismic image +label for a given index
  def __getitem__(self,index):
    #if the index is out of bounds, get a random image
    if index not in range (0, len(self.file_list)):
      return self.__getitem__(np.random.randint(0, self.__len__()))
    #define a file ID using the index parameter
    file_id =self.file_list[index]
    #image folder +path
    image_folder = os.path.join(self.root_path, "images")
    images_path =os .path.join(image_folder, file_id +".png")
    #image folder +path
    mask_folder = os.path.join(self.root_path, "masks")
    mask_path =os .path.join(image_folder, file_id +".png")
    #read it , store it in memory as a byte array
    image = np.array(imageio.imread(images_path),dtype=np.uint8)
    mask = np.array(imageio.imread(mask_path),dtype=np.uint8)
    #return image + label
    return image, mask
  
<\Cell_7>
<Cell_8>
#train image +mask data
train_mask = pd.read_csv('train.csv')
#depth data
depth = pd.read_csv('depths.csv')
#training path
train_path = "./"

#list of files
file_list =list(train_mask['id'].values)
#define our dataset using our class
dataset =TGSSaltDataset(train_path, file_list)

<\Cell_8>
<Cell_9>
#function to visualize these images
def plot2x2Array(image, mask):
  #invoke matplotlib!
  f, axarr= plt.subplots(1,2)
  axarr[0].imshow(image)
  axarr[1].imshow(mask)
  axarr[0].grid()
  axarr[1].grid()
  axarr[0].set_title('Image')
  axarr[1].set_title('Mask')
  
<\Cell_9>
<Cell_10>
for i in range(5):
    image, mask = dataset[np.random.randint(0, len(dataset))]
    plot2x2Array(image, mask)


<\Cell_10>
<Cell_11>
plt.figure(figsize = (6,6))
plt.hist(depth['z'], bins=50)
plt.title('Depth distribution')

<\Cell_11>
<Cell_12>
#convert to image
def rleToMask(rleString,height,width):
    #width heigh
    rows,cols = height,width
    try:
        #get numbers
        rleNumbers = [int(numstring) for numstring in rleString.split(' ')]
        #get pairs
        rlePairs = np.array(rleNumbers).reshape(-1,2)
        #create an image
        img = np.zeros(rows*cols,dtype=np.uint8)
        #for each pair
        for index,length in rlePairs:
            #get the pixel value 
            index -= 1
            img[index:index+length] = 255
        
        
        #reshape
        img = img.reshape(cols,rows)
        img = img.T
    
    #else return empty image
    except:
        img = np.zeros((cols,rows))
    
    return img
<\Cell_12>
<Cell_13>
#for measuring how salty an image is 
def salt_proportion(imgArray):
  try:
    unique, counts = np.unique(imgArray, return_counts=True)
    ## the Total number of pixels is 101*101 = 10,201
    return counts[1]/10201.
  except:
    return 0.0
<\Cell_13>
<Cell_14>
train_mask['mask'] = train_mask['rle_mask'].apply(lambda x: rleToMask(x, 101,101))
train_mask['salt_proportion']= train_mask['mask'].apply(lambda x: salt_proportion(x))

<\Cell_14>
<Cell_15>
merged = train_mask.merge(depth, how='left')
merged.head()
<\Cell_15>
<Cell_16>
plt.figure(figsize=(12,6))
plt.scatter(merged['salt_proportion'], merged['z'])
plt.title('Proportion of salt v.depth')
<\Cell_16>
<Cell_17>
print("Correlation:", np.corrcoef(merged['salt_proportion'], merged['z'])[0,1])
<\Cell_17>
<Cell_18>
from keras.models import Model,load_model
from keras.layers import Input
from keras.layers.core import Lambda, RepeatVector, Reshape
from keras.layers.convolutional import Conv2D, Conv2DTranspose
from keras.layers.pooling import MaxPooling2D
from keras.layers.merge import concatenate
from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau
from keras import backend as K
<\Cell_18>
<Cell_19>
im_width =128
im_height =128
border = 5
im_chan=2 #Number of channels:first is original and second cumsum(axis =0)
n_features =1 # Numbers of extra features, like depth
#path_train = '../input/train/'
#path_test = '../input/test/'

<\Cell_19>
<Cell_20>
#Build U-Net model
input_img = Input((im_height, im_width, im_chan), name='img')
input_features = Input((n_features, ), name='feat')

c1 = Conv2D(8,(3,3), activation='relu', padding='same') (input_img)
c1 = Conv2D(8,(3,3),activation= 'relu', padding= 'same') (c1)
p1 = MaxPooling2D((2,2)) (c1)

c2 = Conv2D(16,(3,3), activation='relu', padding='same') (p1)
c2 = Conv2D(8,(3,3),activation= 'relu', padding= 'same') (c2)
p2 = MaxPooling2D((2,2)) (c2)

c3 = Conv2D(8,(3,3), activation='relu', padding='same') (p2)
c3 = Conv2D(8,(3,3),activation= 'relu', padding= 'same') (c3)
p3 = MaxPooling2D((2,2)) (c3)

c4 = Conv2D(8,(3,3), activation='relu', padding='same') (p3)
c4 = Conv2D(8,(3,3),activation= 'relu', padding= 'same') (c4)
p4 = MaxPooling2D((2,2)) (c4)

#join features information in the depthest layer
f_repeat =RepeatVector(8*8)(input_features)
f_conv = Reshape((8,8, n_features))(f_repeat)
p4_feat = concatenate([p4, f_conv], -1)

c5 = Conv2D(128, (3, 3), activation='relu', padding='same') (p4_feat)
c5 = Conv2D(128, (3, 3), activation='relu', padding='same') (c5)

u6 = Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same') (c5)
#check out this skip connection thooooo
u6 = concatenate([u6, c4])
c6 = Conv2D(64, (3, 3), activation='relu', padding='same') (u6)
c6 = Conv2D(64, (3, 3), activation='relu', padding='same') (c6)

u7 = Conv2DTranspose(32, (2, 2), strides=(2, 2), padding='same') (c6)
u7 = concatenate([u7, c3])
c7 = Conv2D(32, (3, 3), activation='relu', padding='same') (u7)
c7 = Conv2D(32, (3, 3), activation='relu', padding='same') (c7)

u8 = Conv2DTranspose(16, (2, 2), strides=(2, 2), padding='same') (c7)
u8 = concatenate([u8, c2])
c8 = Conv2D(16, (3, 3), activation='relu', padding='same') (u8)
c8 = Conv2D(16, (3, 3), activation='relu', padding='same') (c8)

u9 = Conv2DTranspose(8, (2, 2), strides=(2, 2), padding='same') (c8)
u9 = concatenate([u9, c1], axis=3)
c9 = Conv2D(8, (3, 3), activation='relu', padding='same') (u9)
c9 = Conv2D(8, (3, 3), activation='relu', padding='same') (c9)

outputs = Conv2D(1, (1, 1), activation='sigmoid') (c9)

model = Model(inputs=[input_img, input_features], outputs=[outputs])
model.compile(optimizer='adam', loss='binary_crossentropy') #, metrics=[mean_iou]) # The mean_iou metrics seens to leak train and test values...
model.summary()
<\Cell_20>
<Cell_21>
!pip install ipywidgets
<\Cell_21>
<Cell_22>
import sys
from tqdm import tqdm
from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img
from skimage.transform import resize

train_ids = next(os.walk(train_path+"images"))[2]

#Get and Resize train images and masks
X= np.zeros((len(train_ids), im_height, im_width, im_chan),dtype=np.float32)
y = np.zeros((len(train_ids),im_height,im_width,1), dtype=np.float32)
X_feat = np.zeros((len(train_ids),n_features),dtype=np.float32)
print('Getting and resizing train images and masks ...')
sys.stdout.flush()
for n ,id_ in tqdm(enumerate(train_ids), total =len(train_ids)):
    path = train_path
  
  
    #Depth
    #X_feat[n] =depth.loc[id_.replace('.png',''),'z']
  
    #Load X
    img = load_img(path + '/images/' + id_, grayscale =True)
    x_img = img_to_array(img)
    x_img = resize(x_img, (128, 128, 1), mode='constant', preserve_range=True)
    
    # Create cumsum x
    x_center_mean = x_img[border:-border, border:-border].mean()
    x_csum = (np.float32(x_img)-x_center_mean).cumsum(axis=0)
    x_csum -= x_csum[border:-border, border:-border].mean()
    x_csum /= max(1e-3, x_csum[border:-border, border:-border].std())

    # Load Y
    mask = img_to_array(load_img(path + '/masks/' + id_, grayscale=True))
    mask = resize(mask, (128, 128, 1), mode='constant', preserve_range=True)

    # Save images
    X[n, ..., 0] = x_img.squeeze() / 255
    X[n, ..., 1] = x_csum.squeeze()
    y[n] = mask / 255

print('Done!')
    
<\Cell_22>
<Cell_23>
from sklearn.model_selection import train_test_split

X_train, X_valid, X_feat_train, X_feat_valid, y_train, y_valid = train_test_split(X,X_feat, y, test_size=0.15, random_state=42)

<\Cell_23>
<Cell_24>
callbacks = [
    EarlyStopping(patience=5, verbose=1),
    ReduceLROnPlateau(patience=3, verbose=1),
    ModelCheckpoint('model-tgs-salt-1.h5', verbose=1, save_best_only=True, save_weights_only=True)
]

results = model.fit({'img': X_train, 'feat': X_feat_train}, y_train, batch_size=16, epochs=50, callbacks=callbacks,
                    validation_data=({'img': X_valid, 'feat': X_feat_valid}, y_valid))

<\Cell_24>
<Cell_25>

<\Cell_25>

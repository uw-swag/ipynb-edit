<Cell_0>
import pandas as pd
%matplotlib inline 
<\Cell_0>
<Cell_1>
df = pd.read_csv('./data/training_data.csv')
df.head(5)
<\Cell_1>
<Cell_2>
print(df['title'][1])
print('-----')
print(df['description'][1])
print('Published at:', df['publishedAt'][1])
<\Cell_2>
<Cell_3>
# Number of clickbait and non-clickbait articles
df['label'].value_counts()
<\Cell_3>
<Cell_4>
# Plotting the number of author fields that are Null
df['author'].isnull().value_counts().plot('barh')
<\Cell_4>
<Cell_5>
# The number of characters in the description field
df['description'].apply(len).mean()
<\Cell_5>
<Cell_6>
# Comparing the number of description characters in clickbait to news
df['description'].apply(len).groupby(df['label']).mean()
<\Cell_6>
<Cell_7>
# TEST YOUR KNOWLEDGE
# Can you write a one-liner to compute the number of clickbait articles
# written by each author? Hint: you might find the .sum() function helpful!
<\Cell_7>
<Cell_8>
# df['full_content'] = (df.description + df.title)
df['full_content'] = (df.title)
df.head(1)
<\Cell_8>
<Cell_9>
from sklearn.feature_extraction.text import CountVectorizer
vectorizer = CountVectorizer()
<\Cell_9>
<Cell_10>
sentence = ["Literally just 8 really really cute dogs"]
vectorizer.fit(sentence)
print(vectorizer.vocabulary_) # dictionary of words and ids
<\Cell_10>
<Cell_11>
vectorizer.transform(sentence).toarray()
<\Cell_11>
<Cell_12>
sentence = ["OMG 5 truly hilarious dogs ðŸ˜‚"]
vectorizer.transform(sentence).toarray()
<\Cell_12>
<Cell_13>
from sklearn.svm import LinearSVC
svc = LinearSVC()
<\Cell_13>
<Cell_14>
bag_of_words = [
    [1, 5], [1, 4], [2, 6], [4, 2], [3, 4], [2, 1]
]
labels = [1, 1, 1, 0, 0, 0]
<\Cell_14>
<Cell_15>
from utils.plotting import plot_2d_samples
plot_2d_samples(bag_of_words, labels)
<\Cell_15>
<Cell_16>
svc = svc.fit(bag_of_words, labels)
<\Cell_16>
<Cell_17>
from utils.plotting import plot_2d_trained_svc
plot_2d_trained_svc(bag_of_words, labels, svc)
<\Cell_17>
<Cell_18>
svc.predict([[3, 1], [2,4]])
<\Cell_18>
<Cell_19>
steps = (
    ('vectorizer', CountVectorizer()),
    ('classifier', LinearSVC())
)
<\Cell_19>
<Cell_20>
from sklearn.pipeline import Pipeline
pipeline = Pipeline(steps)
<\Cell_20>
<Cell_21>
from sklearn.model_selection import train_test_split
training, testing = train_test_split(
    df,                # The dataset we want to split
    train_size=0.7,    # The proportional size of our training set
    stratify=df.label, # The labels are used for stratification
    random_state=400   # Use the same random state for reproducibility
)
<\Cell_21>
<Cell_22>
training.head(5)
<\Cell_22>
<Cell_23>
print(len(training))
print(len(testing))
<\Cell_23>
<Cell_24>
pipeline = pipeline.fit(training.full_content, training.label)
<\Cell_24>
<Cell_25>
pipeline.predict(["10 things you need to do..."])
<\Cell_25>
<Cell_26>
pipeline.predict(["French election polls show an early lead for Macron."])
<\Cell_26>
<Cell_27>
from sklearn.metrics import accuracy_score
from sklearn.metrics import confusion_matrix
from utils.plotting import plot_confusion_matrix

def evaluate(pipeline, test_samples, test_labels, n=5):
    predicted_label = pipeline.predict(test_samples)

    print("Accuracy = {:.1f}%".format(
        accuracy_score(test_labels, predicted_label) * 100.0
    ))
    
    cm = confusion_matrix(test_labels, predicted_label)
    plot_confusion_matrix(cm, pipeline.classes_)
    
    wrong_predictions = [
        (true_label, pred_label, test_sample)
        for true_label, pred_label, test_sample in zip(test_labels, predicted_label, test_samples) 
        if true_label != pred_label
    ]
    
    print('true_label, pred_label, test_sample')
    print('\n')
    for wrong_prediction in wrong_predictions[:n]:
        print(wrong_prediction)
    
<\Cell_27>
<Cell_28>
evaluate(pipeline, testing.full_content, testing.label)
<\Cell_28>
<Cell_29>
df['full_content'] = (df.description + "\n" + df.title)
training, testing = train_test_split(
    df,                # The dataset we want to split
    train_size=0.7,    # The proportinal size of our training set
    stratify=df.label, # The labels are used for stratification
    random_state=400   # Use the same random state for reproducibility
)
pipeline = pipeline.fit(training.full_content, training.label)
evaluate(pipeline, testing.full_content, testing.label)
<\Cell_29>
<Cell_30>
from sklearn.feature_extraction.text import TfidfVectorizer

steps = (
    ('vectorizer', TfidfVectorizer()),
    ('classifier', LinearSVC())
)

pipeline = Pipeline(steps)

pipeline = pipeline.fit(training.full_content, training.label)
evaluate(pipeline, testing.full_content, testing.label)
<\Cell_30>
<Cell_31>
steps = (
    ('vectorizer', TfidfVectorizer()),
    ('classifier', LinearSVC())
)

pipeline = Pipeline(steps)
<\Cell_31>
<Cell_32>
gs_params = {
    'vectorizer__stop_words': ['english', None],
    'vectorizer__ngram_range': [(1, 1), (1, 2), (2, 2)],
    'classifier__C': [0.001, 0.01, 0.1, 1, 10, 100]
}
<\Cell_32>
<Cell_33>
from sklearn.model_selection import GridSearchCV
gs = GridSearchCV(pipeline, gs_params, n_jobs=-1)
gs.fit(training.full_content, training.label)
<\Cell_33>
<Cell_34>
print(gs.best_params_)
<\Cell_34>
<Cell_35>
pipeline = gs.best_estimator_
<\Cell_35>
<Cell_36>
evaluate(pipeline, testing.full_content)
<\Cell_36>
<Cell_37>

<\Cell_37>
<Cell_38>
filename = 'classifiers/clickbait_svc_v1'
<\Cell_38>
<Cell_39>
import pickle
with open(filename, 'wb') as f:
    pickle.dump(pipeline, f)
<\Cell_39>
<Cell_40>

<\Cell_40>
<Cell_41>

<\Cell_41>

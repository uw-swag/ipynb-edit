<Cell_0>



from __future__ import print_function
from nltk.stem.snowball import SnowballStemmer
import string
from nltk.tag import pos_tag
from gensim import corpora, models, similarities
from sklearn.externals import joblib
from sklearn.metrics import euclidean_distances
from sklearn.metrics.pairwise import cosine_similarity
import colorsys

import seaborn as sns
from scipy.sparse import coo_matrix
from sklearn.metrics import silhouette_samples, silhouette_score
from pathlib import Path
import sys
from operator import itemgetter
import time
from tqdm.auto import tqdm
import re

from datetime import datetime
from nltk.corpus import stopwords
from nltk import word_tokenize, sent_tokenize
from nltk.stem import PorterStemmer
from nltk import pos_tag
import nltk
import math
import matplotlib.cm as cm
import matplotlib.pyplot as plt
from matplotlib import figure
from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer
from sklearn.cluster import KMeans
from sklearn import metrics
from IPython.display import display, Markdown

import numpy as np
import pandas as pd
import matplotlib



from google.colab import auth
from googleapiclient.discovery import build
import io
from googleapiclient.http import MediaIoBaseDownload



%matplotlib inline
<\Cell_0>
<Cell_1>
!@#auth.authenticate_user()
<\Cell_1>
<Cell_2>
drive_service = build('drive', 'v3')
<\Cell_2>
<Cell_3>
from pydrive.auth import GoogleAuth
from pydrive.drive import GoogleDrive

gauth = GoogleAuth()
gauth.LocalWebserverAuth()

drive = GoogleDrive(gauth)
<\Cell_3>
<Cell_4>
# Install the PyDrive wrapper & import libraries.
# This only needs to be done once per notebook.
!pip install -U -q PyDrive
from pydrive.auth import GoogleAuth
from pydrive.drive import GoogleDrive
from google.colab import auth
from oauth2client.client import GoogleCredentials

# Authenticate and create the PyDrive client.
# This only needs to be done once per notebook.
auth.authenticate_user()
gauth = GoogleAuth()
gauth.credentials = GoogleCredentials.get_application_default()
drive = GoogleDrive(gauth)

# Download a file based on its file ID.
#
# A file ID looks like: laggVyWshwcyP6kEI-y_W3P8D26sz
file_id = '16ZLV4eX0JB2jVI0CV-X2Sg6QkfOBf2jH'
downloaded = drive.CreateFile({'id': file_id})
print('Downloaded content "{}"'.format(downloaded.GetContentString()))
<\Cell_4>
<Cell_5>
page_token = None
while True:
    response = drive_service.files().list(q="name= 'Text_Files_Trained'",
                                          pageToken=page_token).execute()
    for file in response.get('files', []):
        # Process change
        file_id = file.get('id')
        print('Found Model: %s (%s)' % (file.get('name'), folder_id))
    if page_token is None:
        break

request = drive_service.files().get_media(fileId=file_id)
fh = io.BytesIO()
downloader = MediaIoBaseDownload(fh, request)
done = False
while done is False:
    status, done = downloader.next_chunk()
    print("Download %d%%." % int(status.progress() * 100))
    page_token = response.get('nextPageToken', None)

<\Cell_5>
<Cell_6>
       
                
page_token = None
while True:
    response = drive_service.files().list(q="name= 'Model'",
                                          pageToken=page_token).execute()
    for file in response.get('files', []):
        # Process change
        model_folder_id = file.get('id')
        print('Found Vectorizer: %s (%s)' % (file.get('name'), folder_id))
        
    page_token = response.get('nextPageToken', None)
    if page_token is None:
        break
<\Cell_6>
<Cell_7>

while True:
    response = drive_service.files().list(q="name='s2sFeb14-0130PM.pkl' and '%s' in parents" % model_folder_id ,
                                          spaces='drive',
                                          fields='nextPageToken, files(id, name)',
                                          pageToken=page_token).execute()
    for file in response.get('files', []):
        # Process change
        print('Found Model: %s (%s)' % (file.get('name'), file.get('id')))
        ##TODO BIND Model             
    
    page_token = response.get('nextPageToken', None)
    if page_token is None:
        break
        
while True:
    response = drive_service.files().list(q="name='vecFeb14-0130PM.pkl' and '%s' in parents" % model_folder_id ,
                                          spaces='drive',
                                          fields='nextPageToken, files(id, name)',
                                          pageToken=page_token).execute()
    for file in response.get('files', []):
        # Process change
        print('Found Model: %s (%s)' % (file.get('name'), file.get('id')))
        ##TODO BIND VECTORIZER             
    
    page_token = response.get('nextPageToken', None)
    if page_token is None:
        break
<\Cell_7>
<Cell_8>
page_token = None
text_ids = []
names = []
while True:
    response = drive_service.files().list(q="mimeType='text/plain' and '%s' in parents" % folder_id ,
                                          spaces='drive',
                                          fields='nextPageToken, files(id, name)',
                                          pageToken=page_token).execute()
    for file in response.get('files', []):
        # Process change
        print('Found file: %s (%s)' % (file.get('name'), file.get('id')))
        text_ids.append(file.get('id'))
        name = file.get('name').split(".")[0]
        names.append(name.replace("_", " "))
    page_token = response.get('nextPageToken', None)
    if page_token is None:
        break
print('Found %s files' % len(text_ids))
<\Cell_8>
<Cell_9>
while True:
    response = drive_service.files().list(q="%s in parents" % 'Model' ,
                                          spaces='drive',
                                          fields='nextPageToken, files(id, name)',
                                          pageToken=page_token).execute()
    for file in response.get('files', []):
        # Process change
        print('Found file: %s (%s)' % (file.get('name'), file.get('id')))

    page_token = response.get('nextPageToken', None)
    if page_token is None:
        break
<\Cell_9>
<Cell_10>
doclist = []

for file_id in tqdm(text_ids, desc="Downloading Files"):
  request = drive_service.files().get_media(fileId=file_id)
  downloaded = io.BytesIO()
  downloader = MediaIoBaseDownload(downloaded, request)
  done = False
  while done is False:
    # _ is a placeholder for a progress object that we ignore.
    # (Our file is small, so we skip reporting progress.)
    _, done = downloader.next_chunk()

  downloaded.seek(0)
  doclist.append(downloaded.read().decode('Windows-1252'))
<\Cell_10>
<Cell_11>
doclist[:5]
<\Cell_11>
<Cell_12>
def transform_tokens(doclist):
    nltk.download('punkt')

    token_list = []
    for doc in tqdm(doclist, desc="Tokenizing", leave=True):
        dirty_tokens = nltk.sent_tokenize(doc)
        token_list += [dirty_tokens]
    return token_list
<\Cell_12>
<Cell_13>
def transform_filtered(token_list, doclist, names):
    nltk.download('stopwords')
    nltk.download('averaged_perceptron_tagger')
    punc = ['.', ',', '"', "'", '?', '!', ':',
            ';', '(', ')', '[', ']', '{', '}', "%"]
    more_stops = ['\\t\\t\\t',
                  '\\t\\t\\', '\\t\\t\\t',
                  '<U+25CF>', '[1]', 'feff', '1a', 'item']
    maybe_bad_stops = ['may', 'could',  'contents',
                       'table', 'time', '25cf', 'factors', 'risk',
                       ]
    global Stopwords_list
    Stopwords_list = stopwords.words(
        'english') + more_stops + punc + maybe_bad_stops
    filtered_tokens = []
    #WE CAN RUN 1 or many docs at once#
    names_list = []
    if len(names) > 1:
        if len(token_list) != len(doclist):
            token_list = [token_list]
        index = 0

        for tokens in tqdm(token_list, desc="Filtering Documents"):
            filtered_docs = []
            name = names[index]
            for token in tqdm(tokens, desc="Filtering Words", leave=False):
                if re.search(r'\d{1,}', token):  # getting rid of digits
                    pass
                else:
                    #                 NNP proper noun, singular ‘Harrison’
                    #                 NNPS proper noun, plural ‘Americans’
                    if token not in Stopwords_list:
                        if pos_tag(token) != 'NNP' and pos_tag(token) != 'NNPS':
                            filtered_docs.append(token.lower())
                        else:
                            filtered_docs.append('proper_noun')
                        names_list.append(name)
            index += 1
            filtered_tokens.append(filtered_docs)
        else:
            for token in tqdm(tokens, desc="Filtering Words", leave=False):
                if re.search(r'\d{1,}', token):  # getting rid of digits
                    pass
                else:
                    #                 NNP proper noun, singular ‘Harrison’
                    #                 NNPS proper noun, plural ‘Americans’
                    if token not in Stopwords_list:
                        if pos_tag(token) != 'NNP' and pos_tag(token) != 'NNPS':
                            filtered_docs.append(token.lower())
                        else:
                            filtered_docs.append('proper_noun')
    return filtered_tokens, names_list
<\Cell_13>
<Cell_14>
def transform_stemming(filtered_tokens):
    stemmed = []
    for doc in filtered_tokens:
        for token in doc:
            stemmed.append(PorterStemmer().stem(token))
            # stemmed.append(LancasterStemmer().stem(token))
            # stemmed.append(SnowballStemmer('english').stem(token))

    return stemmed
<\Cell_14>
<Cell_15>
def transform_vectorize(stemmed, smallest_ngram, largest_ngram):

    vectorizer = TfidfVectorizer(stop_words=Stopwords_list,
                                 ngram_range=(smallest_ngram, largest_ngram), max_df=0.55, min_df=0.01)
#     vectorizer = CountVectorizer(stop_words=Stopwords_list,
#                                  ngram_range=(smallest_ngram, largest_ngram), max_df=0.75, min_df=0.01)
    sparseMatrix = vectorizer.fit_transform(stemmed)
    return sparseMatrix, vectorizer
<\Cell_15>
<Cell_16>
tokens = transform_tokens(doclist)
filtered_tokens, names_list = transform_filtered(tokens, doclist, names)
stemmed = transform_stemming(filtered_tokens)
<\Cell_16>
<Cell_17>
largest_ngram = 15
smallest_ngram = 1
largest_ngram = len(max(tokens, key=len))
print(smallest_ngram, largest_ngram)
max(tokens[0], key=len)
<\Cell_17>
<Cell_18>
%time sparseMatrix, vectorizer = transform_vectorize(stemmed, smallest_ngram, largest_ngram)
<\Cell_18>
<Cell_19>
vectorizer
<\Cell_19>
<Cell_20>
vectorizer.vocabulary_
<\Cell_20>
<Cell_21>
import matplotlib.pyplot as plt


def plot_coo_matrix(m):
    if not isinstance(m, coo_matrix):
        m = coo_matrix(m)
    fig = plt.figure()
    ax = fig.add_subplot(111, facecolor='black')
    ax.plot(m.col, m.row, 's', color='white', ms=1)
    ax.set_xlim(0, m.shape[1])
    ax.set_ylim(0, m.shape[0])
    ax.set_aspect('equal')
    for spine in ax.spines.values():
        spine.set_visible(False)
    ax.invert_yaxis()
    ax.set_aspect('equal')
    ax.set_xticks([])
    ax.set_yticks([])
    return ax
<\Cell_21>
<Cell_22>
sparseMatrix
# With this, each column in the matrix represents a word in the vocabulary
# each row represents the document in our dataset
# where the values in this case are the word counts or Doc Freq if we are using tfidf vectorizer
<\Cell_22>
<Cell_23>
plot_coo_matrix(sparseMatrix).figure.show()
# This is causing mem issues 2/12/19
<\Cell_23>
<Cell_24>
# from sklearn.model_selection import GridSearchCV
# model = KMeans(init='k-means++', random_state=42, n_init=15
#                    )
# param_grid = {'max_iter': [10, 50, 100, 150, 200, 250, 300, 350, 400, 500, 1000],
#              'n_clusters': [25,30, 33, 35],
#              }
# grid = GridSearchCV(model, param_grid, verbose=3, n_jobs=8)
# grid.fit(sparseMatrix)

# lids = model.cluster_centers_

# score = model.score(sparseMatrix)
# silhouette_score = metrics.silhouette_score(sparseMatrix, labels, metric='euclidean')
<\Cell_24>
<Cell_25>
# List the best parameters for this dataset
# print(grid.best_params_)
# List the best score
# print(grid.best_score_)
<\Cell_25>
<Cell_26>
def estimator_cluster(sparseMatrix, vectorizer):
    truek = 35  # FROM GRID SEARCH
    model = KMeans(n_clusters=truek, init='k-means++',
                   max_iter=50, n_init=1, random_state=42,
                   )
    model.fit(sparseMatrix)

    model_time = datetime.now().strftime("%b%d-%I%M%p")
    joblib.dump(model,  f'../Data/Outputs/s2s{model_time}.pkl')
    joblib.dump(vectorizer,  f'../Data/Outputs/vec{model_time}.pkl')
    order_centroids = model.cluster_centers_.argsort()[:, ::-1]
    terms = vectorizer.get_feature_names()

    return terms, order_centroids, model, truek, model_time
<\Cell_26>
<Cell_27>
# THIS TAKES ~~ 1m per 100 documents
%time terms, order_centroids, model, truek, model_time = estimator_cluster(sparseMatrix, vectorizer)
<\Cell_27>
<Cell_28>
def estimator_ppscore(model):
    labels = model.labels_
    centroids = model.cluster_centers_

    print(f"Model Generated at {model_time}")

    print("Cluster id labels for inputted data")
    print(labels)
    print("Centroids data")
    #print (centroids)

    kmeans_score = model.score(sparseMatrix)
    print("Score (Opposite of the value of X on the K-means objective, \n",
          "which is Sum of distances of samples to their closest cluster center):")
    print(kmeans_score)

    silhouette_score = metrics.silhouette_score(
        sparseMatrix, labels, metric='euclidean')

    print("Silhouette_score: ")
    print(silhouette_score)
    return kmeans_score, silhouette_score


estimator_ppscore(model)
<\Cell_28>
<Cell_29>
# using joblib to pickle model
def estimator_load_model(model_time):
    model = joblib.load(f'../Data/Outputs/s2s{model_time}.pkl')
    vectorizer = joblib.load(f'../Data/Outputs/vec{model_time}.pkl')
    return model, vectorizer


# TODO IMPORTANT RETURN VECTORIZER
model, vectorizer = estimator_load_model('Feb14-0130PM')
<\Cell_29>
<Cell_30>
nclosest_words_to_show = 10
for i in range(truek):
    print('Cluster %d:' % i),
    for ind in order_centroids[i, :nclosest_words_to_show]:
        print(' %s' % terms[ind])
<\Cell_30>
<Cell_31>
def estimator_predict_string(string):
    empty_list = []
    print('Input String: %s' % string)
    print('\n')
    print('Prediction:')

    X = vectorizer.transform([string])
    predicted = model.predict(X)
    print('kmeans prediction: %s' % predicted)
    print("closest cluster centers :")
    for ind in order_centroids[predicted[0], :5]:
        print(' %s' % terms[ind])
    return X
<\Cell_31>
<Cell_32>
estimator_predict_string('The hackers stole all our bitcoin!')
<\Cell_32>
<Cell_33>
terms
<\Cell_33>
<Cell_34>
# SUMMARIZATION OF CORPORATE RISK FACTOR DISCLOSURE THROUGH TOPIC MODELING by Bao, Datta
strings = [
    'Topic 0: investment, property, distribution, interest, agreement',
    'Topic 1: regulation, change, law, financial, operation, tax, accounting ',
    'Topic 2: gas, price, oil, natural, operation, production Input prices risks ',
    'Topic 3: stock, price, share, market, future, dividend, security, stakeholder ',
    'Topic 4: cost, regulation, environmental, law, operation, liability',
    'Topic 5: control, financial, internal, loss, reporting, history ',
    'Topic 6: financial, litigation, operation, condition, action, legal, liability, regulatory, claim, lawsuit'
    'Topic 7: competitive, industry, competition, highly',
    'Topic 8: cost, operation, labor, operating, employee, increase, acquisition ',
    'Topic 9: product, candidate, development, approval, clinical, regulatory',
    'Topic 10: tax, income, asset, net, goodwill, loss, distribution, impairment, intangible ',
    'Topic 11: interest, director, officer, trust, combination, share, conflict ',
    'Topic 12: product, liability, claim, market, insurance, sale, revenue Potential defects in products',
    'Topic 13: loan, real, estate, investment, property, market, loss, portfolio ',
    'Topic 14: personnel, key, retain, attract, management, employee ',
    'Topic 15: stock, price, operating, stockholder, fluctuate, interest, volatile  ',
    'Topic 16: acquisition, growth, future, operation, additional, capital, strategy ',
    'Topic 17: condition, economic, financial, market, industry, change, affected, downturn, demand Macroeconomic risks ',
    'Topic 18: system, service, information, failure, product, operation, software, network, breach, interruption Disruption of operations'
    'Topic 19: cost, contract, operation, plan, increase, pension, delay',
    'Topic 20: customer, product, revenue, sale, supplier, relationship, key, portion, contract, manufacturing, rely Rely on few large customers',
    'Topic 21: property, intellectual, protect, proprietary, technology, patent, protection, harm',
    'Topic 22: product, market, service, change, sale, demand, successfully, technology, competition Volatile demand and results',
    'Topic 23: provision, law, control, change, stock, prevent, stockholder, Delaware, charter, delay, bylaw',
    'Topic 24: regulation, government, change, revenue, contract, law, service',
    'Topic 25: capital, credit, financial, market, cost, operation, rating, access, liquidity, downgrade ',
    'Topic 26: debt, indebtedness, cash, obligation, financial, credit, ',
    'Topic 27: operation, international, foreign, currency, rate, fluctuation',
    'Topic 28: loss, insurance, financial, loan, reserve, operation, cover',
    'Topic 29: operation, natural, facility, disaster, event, terrorist, weather ']
topics = [topic.split(":")[1] for topic in strings]
<\Cell_34>
<Cell_35>
targets = {
    "Shareholder’s interest risk": topics[0],
    "Regulation changes(accounting)": topics[1],
    "Stakeholder’s profit": topics[2],
    "Regulation changes(environment)": topics[3],
    "Legal Risks": topics[4],
    "Financial condition risks ": topics[5],
    " Potential/Ongoing Lawsuits": topics[6],
    "market Competition risks": topics[7],
    "**Labor cost ": topics[8],
    " New product introduction risks ": topics[9],
    "**Accounting,  +Restructuring risks ": topics[10],
    "**Management": topics[11],
    " Potential defects in products": topics[12],
    "**Investment": topics[13],
    "Human resource risks": topics[13],
    "Volatile stock price risks": topics[14],
    "Merger & Acquisition risks": topics[15],
    " +Industry is cyclical": topics[16],
    " **Postpone ":  topics[17],
    " +Infrastructure risks": topics[18],
    "+Suppliers risks +Downstream risks": topics[19],
    "license Intellectual property risks": topics[20],
    "+Licensing related risks' ": topics[21],
    "+ Competition risks ": topics[22],
    "*Potential/Ongoing Lawsuits*": topics[23],
    "Regulation changes": topics[24],
    "Credit risks": topics[25],
    "covenant Funding risks ": topics[26],
    "International risks": topics[27],
    #     "Insurance" : topics[28],
    #     "Catastrophes" : topics[29]
}
<\Cell_35>
<Cell_36>
for topic in topics:
    print(topic)
    estimator_predict_string(topic)
<\Cell_36>
<Cell_37>
doclist, names = make_pipeline(15, istesting=True)
<\Cell_37>
<Cell_38>
names
<\Cell_38>
<Cell_39>
def estimator_predict_document(document, name):
    dictionary_list = []
    for counter, sentence in enumerate(document.split(".")):
        if len(sentence) != 0:
            vector_matrix = vectorizer.transform([sentence])
            predicted_label = model.predict(vector_matrix)
            sentence_len = len(sentence.split(" "))
            sentence_info = {'company': name, 'sentence#': counter, 'text': sentence,
                             'wordcount': sentence_len, 'label': predicted_label[0]}
            dictionary_list.append(sentence_info)
    dataframe = pd.DataFrame(dictionary_list)
    dataframe["% of total"] = dataframe['wordcount'] / \
        sum(dataframe['wordcount'])
#         (name, sentence, predicted_label)
    return(dataframe)
<\Cell_39>
<Cell_40>
frames = []
for document, name in zip(doclist, names):
    frame = estimator_predict_document(document, name)
    frames.append(frame)

muliple_company_frame = pd.concat(frames)
muliple_company_frame.head()
<\Cell_40>
<Cell_41>
grouped_frame = muliple_company_frame.groupby(
    ['company', 'label']).agg({'% of total': 'sum'}).reset_index()
grouped_frame.head()
<\Cell_41>
<Cell_42>
def prep_for_heatmap(muliple_company_frame):
    company_clusters = muliple_company_frame.groupby(['label', 'company']).agg(
        {'% of total': 'sum'}).unstack(level='company').fillna(0).T

    company_clusters = company_clusters.reset_index(level=0, drop=True)
    return company_clusters


company_clusters = prep_for_heatmap(muliple_company_frame)
company_clusters.head()
<\Cell_42>
<Cell_43>
def plot_heatmap(company_clusters):
    fig2, ax2 = plt.subplots(figsize=(20, 20))
    cmap = sns.light_palette('blue', as_cmap=True)

    sns.heatmap(company_clusters, ax=ax2, cmap=cmap)

    ax2.set_xlabel('Label', fontdict={'weight': 'bold', 'size': 14})
    ax2.set_ylabel('Company', fontdict={'weight': 'bold', 'size': 14})
    for label in ax2.get_xticklabels():
        label.set_size(16)
        label.set_weight("bold")
    for label in ax2.get_yticklabels():
        label.set_size(16)
        label.set_weight("bold")
    plt.savefig("../Data/Outputs/Heatmap.jpg", dpi=None, facecolor='w', edgecolor='w',
                orientation='portrait', papertype=None, format=None,
                transparent=False, bbox_inches=None, pad_inches=0.1,
                frameon=None, metadata=None)


plot_heatmap(company_clusters)
<\Cell_43>
<Cell_44>
company_clusters.T.head(35)
<\Cell_44>
<Cell_45>
#  https://stackoverflow.com/questions/876853/generating-color-ranges-in-python


def get_N_HexCol(N):
    HSV_tuples = [(x * 1.0 / N, 0.5, 0.5) for x in range(N)]
    hex_out = []
    for rgb in HSV_tuples:
        rgb = map(lambda x: int(x * 255), colorsys.hsv_to_rgb(*rgb))
        hex_out.append('#%02x%02x%02x' % tuple(rgb))
    return hex_out


colormap = get_N_HexCol(truek)
<\Cell_45>
<Cell_46>
for label in range(truek):
    color = colormap[label]
    display(Markdown(f'<font color="{color}">Cluster {label}</font>'))
<\Cell_46>
<Cell_47>
doc_index = 0
company = muliple_company_frame['company'].unique()[doc_index]
companyFrame = muliple_company_frame[muliple_company_frame['company'] == company]
for text, label in zip(companyFrame['text'], companyFrame['label']):
    color = colormap[label]
    display(Markdown(f'<font color="{color}">' +
                     text + f'  ({label})' + '</font>'))
<\Cell_47>
<Cell_48>

<\Cell_48>
<Cell_49>
# global bad_labels
# bad_labels = []
# def mean_supercluster(dataframe):
#     for index, value in zip(dataframe.mean(axis=0).index, dataframe.mean(axis=0)):
#         if value > .75:
#             if index not in bad_labels:
#                 bad_labels.append(index)
#             return index, value

# supercluster_label, value = mean_supercluster(prep_for_heatmap(muliple_company_frame))
# print(supercluster_label, value)
# print(bad_labels)
# while type(supercluster_label) == int and len(bad_labels) < truek:

#     to_recluster = muliple_company_frame['label']==supercluster_label
#     recluster_frame = muliple_company_frame[to_recluster]
#     index_values = recluster_frame['text'].index.values
#     texts = recluster_frame['text'].values
#     for index_value, text in zip(index_values, texts):
#         X = vectorizer.transform([text])
#         distances = euclidean_distances(X, model.cluster_centers_)
#         for label in bad_labels:
#             distances[0][label] = 99
#         min_value = min(distances[0])
#         new_mins = [i for i, x in enumerate(distances[0]) if x == min_value]

#         new_min = new_mins[0]
#         muliple_company_frame.at[index_value, 'label'] = new_min

#     supercluster_label, value = mean_supercluster(prep_for_heatmap(muliple_company_frame))
#     print(supercluster_label, value)
#     print(bad_labels)
#     if value < 90:
#         plot_heatmap(prep_for_heatmap(muliple_company_frame))
<\Cell_49>
<Cell_50>

<\Cell_50>
<Cell_51>

<\Cell_51>

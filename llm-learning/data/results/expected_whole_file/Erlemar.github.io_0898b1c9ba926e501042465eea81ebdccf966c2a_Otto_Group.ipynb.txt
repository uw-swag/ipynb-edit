<Cell_0>
import pandas as pd
import sklearn
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import log_loss
from sklearn.calibration import CalibratedClassifierCV
import numpy as np
import xgboost as xgb
<\Cell_0>
<Cell_1>
#Read data. Input the path to the files instead of "../input".
data = pd.read_csv('../input/train.csv')
test = pd.read_csv('../input/test.csv')
<\Cell_1>
<Cell_2>
#Define data for modelling.
X_train = data.drop('id', axis=1)
X_train = X_train.drop('target', axis=1)
Y_train = LabelEncoder().fit_transform(data.target.values)
X_test = test.drop('id', axis=1)
<\Cell_2>
<Cell_3>
#Splitting for crossvalidation
Xtrain, Xtest, ytrain, ytest = train_test_split(X_train, Y_train, test_size=0.20, random_state=36)
<\Cell_3>
<Cell_4>
clf = RandomForestClassifier(n_estimators=300, n_jobs=-1, criterion = 'gini')
#CalibratedClassifierCV - probability calibration with cross-validation.
calibrated_clf = CalibratedClassifierCV(clf, method='isotonic', cv=5)
calibrated_clf.fit(Xtrain, ytrain)
y_val = calibrated_clf.predict_proba(Xtest)
y_submit = calibrated_clf.predict_proba(X_test)
print("Loss on validation set: ", log_loss(ytest, y_val, eps=1e-15, normalize=True))
<\Cell_4>
<Cell_5>
#I decided to add XGBoost to improve the model and it helped.
params = {"objective": "multi:softprob", "num_class": 9}
gbm = xgb.train(params, xgb.DMatrix(X_train, Y_train), 20)
Y_pred = gbm.predict(xgb.DMatrix(X_test))
<\Cell_5>
<Cell_6>
#RF gave better results, so it has higher weight.
y = 0.2*Y_pred + 0.8*y_submit
<\Cell_6>
<Cell_7>
sample = pd.read_csv('../input/sampleSubmission.csv')
preds = pd.DataFrame(y, index=sample.id.values, columns=sample.columns[1:])
preds.to_csv('Otto.csv', index_label='id')
<\Cell_7>

<Cell_0>
import datetime
import pandas as pd
import numpy as np
import requests
import zipfile
import io
import json

from sklearn import datasets, ensemble, model_selection
from scipy.stats import anderson_ksamp

from evidently.dashboard import Dashboard
from evidently.pipeline.column_mapping import ColumnMapping
from evidently.dashboard.tabs import DataDriftTab, NumTargetDriftTab, RegressionPerformanceTab
from evidently.options import DataDriftOptions
from evidently.model_profile import Profile
from evidently.model_profile.sections import DataDriftProfileSection, RegressionPerformanceProfileSection
<\Cell_0>
<Cell_1>
import warnings
warnings.filterwarnings('ignore')
warnings.simplefilter('ignore')
<\Cell_1>
<Cell_2>
content = requests.get("https://archive.ics.uci.edu/ml/machine-learning-databases/00275/Bike-Sharing-Dataset.zip").content
with zipfile.ZipFile(io.BytesIO(content)) as arc:
    raw_data = pd.read_csv(arc.open("hour.csv"), header=0, sep=',', parse_dates=['dteday']) 
<\Cell_2>
<Cell_3>
raw_data.head()
<\Cell_3>
<Cell_4>
raw_data.index = raw_data.apply(lambda row: datetime.datetime.combine(row.dteday.date(), datetime.time(row.hr)),
                                axis=1)
<\Cell_4>
<Cell_5>
raw_data.head()
<\Cell_5>
<Cell_6>
target = 'cnt'
prediction = 'prediction'
numerical_features = ['temp', 'atemp', 'hum', 'windspeed', 'mnth', 'hr', 'weekday']
categorical_features = ['season', 'holiday', 'workingday', ]#'weathersit']
<\Cell_6>
<Cell_7>
reference = raw_data.loc['2011-01-01 00:00:00':'2011-01-28 23:00:00']
current = raw_data.loc['2011-01-29 00:00:00':'2011-02-28 23:00:00']
<\Cell_7>
<Cell_8>
X_train, X_test, y_train, y_test = model_selection.train_test_split(
    reference[numerical_features + categorical_features],
    reference[target],
    test_size=0.3
)
<\Cell_8>
<Cell_9>
regressor = ensemble.RandomForestRegressor(random_state = 0, n_estimators = 50)
<\Cell_9>
<Cell_10>
regressor.fit(X_train, y_train)
<\Cell_10>
<Cell_11>
preds_train = regressor.predict(X_train)
preds_test = regressor.predict(X_test)
<\Cell_11>
<Cell_12>
X_train['target'] = y_train
X_train['prediction'] = preds_train

X_test['target'] = y_test
X_test['prediction'] = preds_test
<\Cell_12>
<Cell_13>
column_mapping = ColumnMapping()

column_mapping.target = 'target'
column_mapping.prediction = 'prediction'
column_mapping.numerical_features = numerical_features
column_mapping.categorical_features = categorical_features
<\Cell_13>
<Cell_14>
regression_perfomance_dashboard = Dashboard(tabs=[RegressionPerformanceTab()])
regression_perfomance_dashboard.calculate(X_train.sort_index(), X_test.sort_index(), 
                                          column_mapping=column_mapping)
<\Cell_14>
<Cell_15>
regression_perfomance_dashboard.show()
<\Cell_15>
<Cell_16>
regressor.fit(reference[numerical_features + categorical_features], reference[target])
<\Cell_16>
<Cell_17>
column_mapping = ColumnMapping()

column_mapping.target = target
column_mapping.prediction = prediction
column_mapping.numerical_features = numerical_features
column_mapping.categorical_features = categorical_features
<\Cell_17>
<Cell_18>
ref_prediction = regressor.predict(reference[numerical_features + categorical_features])
reference['prediction'] = ref_prediction
<\Cell_18>
<Cell_19>
regression_perfomance_dashboard = Dashboard(tabs=[RegressionPerformanceTab(verbose_level=0)])
regression_perfomance_dashboard.calculate(reference, None, column_mapping=column_mapping)
regression_perfomance_dashboard.show()
<\Cell_19>
<Cell_20>
current_prediction = regressor.predict(current[numerical_features + categorical_features])
current['prediction'] = current_prediction
<\Cell_20>
<Cell_21>
regression_perfomance_dashboard.calculate(reference, current.loc['2011-01-29 00:00:00':'2011-02-07 23:00:00'], 
                                            column_mapping=column_mapping)
regression_perfomance_dashboard.show()
<\Cell_21>
<Cell_22>
RegressionPerformanceTab.list_widgets()
<\Cell_22>
<Cell_23>
regression_perfomance_dashboard = Dashboard(tabs=[RegressionPerformanceTab(include_widgets=[
    'Regression Model Performance Report.',
    'Reference: Model Quality (+/- std)',
    'Current: Model Quality (+/- std)',
    'Current: Error (Predicted - Actual)',
    'Current: Error Distribution',
])])
regression_perfomance_dashboard.calculate(reference, current.loc['2011-02-07 00:00:00':'2011-02-14 23:00:00'], 
                                            column_mapping=column_mapping)
regression_perfomance_dashboard.show()
<\Cell_23>
<Cell_24>
regression_perfomance_dashboard.calculate(reference, current.loc['2011-02-15 00:00:00':'2011-02-21 23:00:00'], 
                                            column_mapping=column_mapping)
regression_perfomance_dashboard.show()
<\Cell_24>
<Cell_25>
column_mapping_drift = ColumnMapping()

column_mapping_drift.target = target
column_mapping_drift.prediction = prediction
column_mapping_drift.numerical_features = numerical_features
column_mapping_drift.categorical_features = []
<\Cell_25>
<Cell_26>
data_drift_dashboard = Dashboard(tabs=[DataDriftTab()])
data_drift_dashboard.calculate(reference,
                               current.loc['2011-02-14 00:00:00':'2011-02-21 23:00:00'], 
                               column_mapping_drift
                              )
data_drift_dashboard.show()
<\Cell_26>
<Cell_27>
def anderson_stat_test(reference_data: pd.DataFrame, current_data: pd.DataFrame):
    return anderson_ksamp(np.array([reference_data, current_data]))[2]

options = DataDriftOptions(feature_stattest_func=anderson_stat_test, nbinsx=20, confidence=0.90)
<\Cell_27>
<Cell_28>
the_dashboard = Dashboard(
    tabs=[RegressionPerformanceTab(include_widgets=[
                                    'Regression Model Performance Report.',
                                    'Reference: Model Quality (+/- std)',
                                    'Current: Model Quality (+/- std)',
                                    'Current: Error (Predicted - Actual)',
                                    'Current: Error Distribution',]
                                  ),
          DataDriftTab()],
    options=[options])
                                
the_dashboard.calculate(reference,
                        current.loc['2011-02-14 00:00:00':'2011-02-21 23:00:00'], 
                        column_mapping_drift)

the_dashboard.show()
<\Cell_28>
<Cell_29>
import mlflow
#import mlflow.sklearn
from mlflow.tracking import MlflowClient
<\Cell_29>
<Cell_30>
experiment_batches = [
    ('2011-01-29 00:00:00','2011-02-07 23:00:00'),
    ('2011-02-07 00:00:00','2011-02-14 23:00:00'),
    ('2011-02-15 00:00:00','2011-02-21 23:00:00'),  
]
<\Cell_30>
<Cell_31>
model_profile = Profile(sections=[DataDriftProfileSection(), RegressionPerformanceProfileSection()])
model_profile.calculate(reference, 
                        current.loc[experiment_batches[0][0]:experiment_batches[0][1]],
                        column_mapping=column_mapping_drift)
<\Cell_31>
<Cell_32>
logged_json_profile = json.loads(model_profile.json())
<\Cell_32>
<Cell_33>
logged_json_profile["regression_performance"]["data"]["metrics"]["current"]["mean_error"]
<\Cell_33>
<Cell_34>
logged_json_profile["data_drift"]["data"]["metrics"]["share_drifted_features"]
<\Cell_34>
<Cell_35>
#log into MLflow
client = MlflowClient()

#set experiment
mlflow.set_experiment('Model Quality Evaluation with Evidently')

#generate model profile
model_profile = Profile(sections=[DataDriftProfileSection(), RegressionPerformanceProfileSection()])

#start new run
for date in experiment_batches:
    with mlflow.start_run() as run: #inside brackets run_name='test'
        
        # Log parameters
        mlflow.log_param("begin", date[0])
        mlflow.log_param("end", date[1])

        # Get metrics
        model_profile.calculate(reference, 
                        current.loc[date[0]:date[1]],
                        column_mapping=column_mapping_drift)
        logged_json_profile = json.loads(model_profile.json())
        
        me = logged_json_profile["regression_performance"]["data"]["metrics"]["current"]["mean_error"]
        mae = logged_json_profile["regression_performance"]["data"]["metrics"]["current"]["mean_abs_error"]
        drift_share = logged_json_profile["data_drift"]["data"]["metrics"]["share_drifted_features"]
        
        # Log metrics
        mlflow.log_metric('me', round(me, 3))
        mlflow.log_metric('mae', round(mae, 3))
        mlflow.log_metric('drift_share', round(drift_share, 3))

        print(run.info)
<\Cell_35>
<Cell_36>
#run MLflow UI (NOT recommented! It will be more convinient to run it directly from the TERMINAL)
#!mlflow ui
<\Cell_36>
